{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CrawlerBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 129\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 20\n",
      "        Vector Action descriptions: , , , , , , , , , , , , , , , , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Crawler_Windows_x86_64/Crawler.exe\")\n",
    "# env = UnityEnvironment(file_name=\"./Reacher_Windows_x86_64/Reacher.exe\")\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "states = env_info.vector_observations\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size = states.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to plot the progress of the agent's score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(scores):\n",
    "    # plot the scores\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents_maddpg.storage_sac import Storage\n",
    "def train(agent, scores=[], n_episodes=500, train_mode=True, episode_start=1, start_to_learn_at = 0):\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    for s in scores[-100:]:\n",
    "        scores_window.append(s)\n",
    "    \n",
    "    frame_no = 0\n",
    "    for i_episode in range(episode_start, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        scores_one_episode = np.zeros(num_agents)\n",
    "        while True:\n",
    "            frame_no += 1\n",
    "            if(frame_no < start_to_learn_at):\n",
    "                actions = agent.test(states)\n",
    "            else:\n",
    "                actions = agent.act(states)              # select an action (for each agent)\n",
    "                \n",
    "            env_info = env.step(np.clip(actions * 1.001, -1, 1))[brain_name]              # send all actions to the environment\n",
    "            next_states = env_info.vector_observations                            # get next state (for each agent)\n",
    "            rewards = env_info.rewards                                            # get reward (for each agent)\n",
    "            dones = env_info.local_done                                           # see if episode finished\n",
    "            \n",
    "            if(frame_no < start_to_learn_at):\n",
    "                agent.add_to_memory(states, actions, rewards, next_states, dones) # add only to memory                      \n",
    "            else:\n",
    "                agent.step(states, actions, rewards, next_states, dones)          # learn\n",
    "            states = next_states                                                  # roll over states to next time step\n",
    "            \n",
    "            scores_one_episode += rewards\n",
    "            if np.any(dones):                                                     # exit loop if episode finished\n",
    "                break\n",
    "                       \n",
    "        score = np.average(scores_one_episode)\n",
    "        scores.append(score)\n",
    "        scores_window.append(score)\n",
    "        mean_100 = np.mean(scores_window)\n",
    "        \n",
    "        if i_episode % 50 == 0:\n",
    "            print('\\rEpisode {}\\tAvg: {:.3f}\\tMin: {:.3f}\\tMax: {:.3f}\\talpha: {:.3f}\\tPLoss: {:.3f}\\tCLoss: {:.3f}\\tEst: {:.3f}'.\n",
    "                  format(i_episode, mean_100, \n",
    "                             np.min(np.array(scores_window)[-50:]),\n",
    "                             np.max(np.array(scores_window)[-50:]),\n",
    "                             agent.network.log_alpha.exp().cpu().detach().numpy().item(),\n",
    "                             agent.policy_loss, np.mean(agent.critics_losses),\n",
    "                             agent.estimation))\n",
    "            Storage.save(\"weights\\SAC\\eps_{}_avg_{:.3f}.pth\".format(i_episode, mean_100), scores, agent=agent)\n",
    "            \n",
    "        if len(scores_window) >= 100 and np.mean(scores_window)>=2000:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.3f}'.format(i_episode, mean_100))\n",
    "            Storage.save(\"weights\\SAC\\final.pth\", scores, agent=agent)\n",
    "            break\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import agents_maddpg\n",
    "import random\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(354)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Consider data in the future with higher GAMMA\n",
    "2. Bigger Q_Number\n",
    "3. Amplifier of move before sending to environment\n",
    "4. 3 loops of update every 2 timesteps\n",
    "5. Bigger hidden layer 128 instead of 112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents_maddpg.model import TanhGaussianActorCritic\n",
    "import torch.nn.functional as F\n",
    "agent = Storage.new( TanhGaussianActorCritic, states.shape[1], action_size, device,  \n",
    "                    memory_size=int(1e5),\n",
    "                    batch_size=16,\n",
    "                    ACTIVATION = F.leaky_relu,\n",
    "                    TAU=1e-2,\n",
    "                    LR_CRITIC = 1e-4,\n",
    "                    LR_ACTOR = 1e-4,\n",
    "                    LR_ALPHA = 1e-4,\n",
    "                    UPDATE_EVERY=2,\n",
    "                    TRANSFER_EVERY=1,\n",
    "                    UPDATE_LOOP=3,\n",
    "                    GAMMA=0.992,\n",
    "                    TARGET_ENTROPY=2,\n",
    "                    Q_NUMBER = 3,\n",
    "                    WEIGHT_DECAY = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Storage.save(\"temp.ckp\", [], agent)\n",
    "loaded, scores = Storage.load(\"temp.ckp\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50\tAvg: 1.464\tMin: -8.353\tMax: 15.257\talpha: 0.774\tPLoss: -264.682\tCLoss: 132.077\tEst: 255.743\n",
      "Episode 100\tAvg: 2.123\tMin: -6.902\tMax: 13.754\talpha: 0.532\tPLoss: -355.901\tCLoss: 137.241\tEst: 352.960\n",
      "Episode 150\tAvg: 4.202\tMin: -3.879\tMax: 19.025\talpha: 0.361\tPLoss: -343.427\tCLoss: 63.450\tEst: 341.852\n",
      "Episode 200\tAvg: 4.228\tMin: -9.098\tMax: 11.883\talpha: 0.266\tPLoss: -301.306\tCLoss: 93.403\tEst: 298.919\n",
      "Episode 250\tAvg: 4.777\tMin: -6.602\tMax: 20.546\talpha: 0.175\tPLoss: -242.357\tCLoss: 49.654\tEst: 241.328\n",
      "Episode 300\tAvg: 4.744\tMin: -14.370\tMax: 20.704\talpha: 0.132\tPLoss: -201.047\tCLoss: 44.774\tEst: 199.968\n",
      "Episode 350\tAvg: 2.520\tMin: -5.901\tMax: 9.455\talpha: 0.106\tPLoss: -164.944\tCLoss: 33.613\tEst: 163.805\n",
      "Episode 400\tAvg: 2.289\tMin: -10.037\tMax: 12.388\talpha: 0.076\tPLoss: -108.193\tCLoss: 13.181\tEst: 107.848\n",
      "Episode 450\tAvg: 1.522\tMin: -3.985\tMax: 8.872\talpha: 0.068\tPLoss: -96.182\tCLoss: 19.965\tEst: 95.674\n",
      "Episode 500\tAvg: 3.312\tMin: -2.219\tMax: 22.777\talpha: 0.055\tPLoss: -63.815\tCLoss: 4.047\tEst: 63.820\n",
      "Episode 550\tAvg: 4.112\tMin: -5.936\tMax: 22.146\talpha: 0.041\tPLoss: -44.405\tCLoss: 7.518\tEst: 44.106\n",
      "Episode 600\tAvg: 5.385\tMin: -2.125\tMax: 29.476\talpha: 0.029\tPLoss: -25.214\tCLoss: 1.249\tEst: 25.062\n",
      "Episode 650\tAvg: 5.714\tMin: -3.568\tMax: 9.916\talpha: 0.025\tPLoss: -21.912\tCLoss: 1.910\tEst: 21.808\n",
      "Episode 700\tAvg: 2.769\tMin: 0.028\tMax: 10.042\talpha: 0.023\tPLoss: -18.343\tCLoss: 2.172\tEst: 18.283\n",
      "Episode 750\tAvg: 3.717\tMin: -0.234\tMax: 26.307\talpha: 0.020\tPLoss: -16.743\tCLoss: 0.501\tEst: 16.731\n",
      "Episode 800\tAvg: 3.676\tMin: -0.476\tMax: 11.574\talpha: 0.019\tPLoss: -15.187\tCLoss: 0.752\tEst: 15.164\n",
      "Episode 850\tAvg: 5.674\tMin: 0.386\tMax: 26.383\talpha: 0.019\tPLoss: -13.482\tCLoss: 1.165\tEst: 13.466\n",
      "Episode 900\tAvg: 9.450\tMin: 0.539\tMax: 25.432\talpha: 0.019\tPLoss: -12.508\tCLoss: 0.511\tEst: 12.645\n",
      "Episode 950\tAvg: 10.449\tMin: 1.366\tMax: 58.151\talpha: 0.020\tPLoss: -16.864\tCLoss: 0.659\tEst: 16.736\n",
      "Episode 1000\tAvg: 10.781\tMin: -0.331\tMax: 31.198\talpha: 0.018\tPLoss: -19.020\tCLoss: 0.292\tEst: 18.997\n",
      "Episode 1050\tAvg: 10.662\tMin: 0.723\tMax: 40.697\talpha: 0.020\tPLoss: -17.461\tCLoss: 0.394\tEst: 17.354\n",
      "Episode 1100\tAvg: 9.824\tMin: -1.154\tMax: 30.685\talpha: 0.022\tPLoss: -16.800\tCLoss: 0.777\tEst: 16.681\n",
      "Episode 1150\tAvg: 11.248\tMin: -0.260\tMax: 49.024\talpha: 0.022\tPLoss: -17.135\tCLoss: 0.308\tEst: 17.124\n",
      "Episode 1200\tAvg: 14.112\tMin: 1.558\tMax: 59.867\talpha: 0.025\tPLoss: -17.768\tCLoss: 2.176\tEst: 17.822\n",
      "Episode 1250\tAvg: 13.353\tMin: 3.949\tMax: 31.908\talpha: 0.026\tPLoss: -17.844\tCLoss: 1.925\tEst: 17.758\n",
      "Episode 1300\tAvg: 13.411\tMin: 0.459\tMax: 32.158\talpha: 0.026\tPLoss: -15.479\tCLoss: 0.329\tEst: 15.411\n",
      "Episode 1350\tAvg: 13.806\tMin: 2.982\tMax: 30.330\talpha: 0.028\tPLoss: -14.774\tCLoss: 0.447\tEst: 14.733\n",
      "Episode 1400\tAvg: 12.489\tMin: 0.454\tMax: 43.224\talpha: 0.027\tPLoss: -13.115\tCLoss: 1.151\tEst: 13.198\n",
      "Episode 1450\tAvg: 13.395\tMin: 1.170\tMax: 36.895\talpha: 0.028\tPLoss: -13.055\tCLoss: 0.401\tEst: 13.004\n",
      "Episode 1500\tAvg: 13.982\tMin: 0.631\tMax: 35.597\talpha: 0.028\tPLoss: -12.993\tCLoss: 0.507\tEst: 12.855\n",
      "Episode 1550\tAvg: 8.619\tMin: -0.784\tMax: 22.059\talpha: 0.028\tPLoss: -11.787\tCLoss: 0.541\tEst: 11.730\n",
      "Episode 1600\tAvg: 9.031\tMin: 0.064\tMax: 31.572\talpha: 0.031\tPLoss: -14.092\tCLoss: 0.955\tEst: 14.264\n",
      "Episode 1650\tAvg: 14.648\tMin: 3.041\tMax: 26.819\talpha: 0.033\tPLoss: -13.041\tCLoss: 5.821\tEst: 12.892\n",
      "Episode 1700\tAvg: 15.831\tMin: 2.735\tMax: 48.456\talpha: 0.033\tPLoss: -13.398\tCLoss: 1.446\tEst: 13.427\n",
      "Episode 1750\tAvg: 15.412\tMin: 1.150\tMax: 33.730\talpha: 0.036\tPLoss: -16.325\tCLoss: 0.880\tEst: 16.299\n",
      "Episode 1800\tAvg: 16.255\tMin: 3.741\tMax: 35.103\talpha: 0.036\tPLoss: -17.657\tCLoss: 0.356\tEst: 17.418\n",
      "Episode 1850\tAvg: 18.030\tMin: 6.271\tMax: 32.622\talpha: 0.037\tPLoss: -14.788\tCLoss: 0.945\tEst: 14.875\n",
      "Episode 1900\tAvg: 18.447\tMin: 1.810\tMax: 44.337\talpha: 0.040\tPLoss: -16.850\tCLoss: 1.673\tEst: 16.832\n",
      "Episode 1950\tAvg: 18.704\tMin: 4.173\tMax: 41.647\talpha: 0.042\tPLoss: -20.274\tCLoss: 0.877\tEst: 20.223\n",
      "Episode 2000\tAvg: 17.507\tMin: 0.034\tMax: 44.622\talpha: 0.046\tPLoss: -18.824\tCLoss: 1.400\tEst: 18.764\n",
      "Episode 2050\tAvg: 16.532\tMin: 1.735\tMax: 36.641\talpha: 0.049\tPLoss: -21.887\tCLoss: 1.035\tEst: 21.841\n",
      "Episode 2100\tAvg: 16.849\tMin: 2.878\tMax: 54.229\talpha: 0.050\tPLoss: -21.406\tCLoss: 2.581\tEst: 21.189\n",
      "Episode 2150\tAvg: 16.938\tMin: 0.514\tMax: 36.359\talpha: 0.050\tPLoss: -19.230\tCLoss: 1.290\tEst: 19.332\n",
      "Episode 2200\tAvg: 15.628\tMin: 2.605\tMax: 37.596\talpha: 0.048\tPLoss: -21.983\tCLoss: 1.089\tEst: 21.757\n",
      "Episode 2250\tAvg: 13.847\tMin: 1.724\tMax: 27.043\talpha: 0.044\tPLoss: -18.554\tCLoss: 2.876\tEst: 18.433\n",
      "Episode 2300\tAvg: 13.425\tMin: 1.478\tMax: 34.638\talpha: 0.044\tPLoss: -20.957\tCLoss: 2.244\tEst: 20.993\n",
      "Episode 2350\tAvg: 14.414\tMin: 2.843\tMax: 32.988\talpha: 0.046\tPLoss: -18.732\tCLoss: 1.438\tEst: 18.830\n",
      "Episode 2400\tAvg: 15.579\tMin: 0.178\tMax: 37.226\talpha: 0.044\tPLoss: -17.948\tCLoss: 1.796\tEst: 17.932\n",
      "Episode 2450\tAvg: 16.723\tMin: 3.560\tMax: 38.706\talpha: 0.047\tPLoss: -20.607\tCLoss: 1.051\tEst: 20.371\n",
      "Episode 2500\tAvg: 18.065\tMin: 4.567\tMax: 38.066\talpha: 0.046\tPLoss: -16.649\tCLoss: 2.084\tEst: 16.676\n",
      "Episode 2550\tAvg: 18.784\tMin: 4.623\tMax: 47.192\talpha: 0.047\tPLoss: -18.172\tCLoss: 1.672\tEst: 18.006\n",
      "Episode 2600\tAvg: 17.546\tMin: 2.053\tMax: 56.766\talpha: 0.050\tPLoss: -20.053\tCLoss: 1.243\tEst: 20.012\n",
      "Episode 2650\tAvg: 18.098\tMin: 3.933\tMax: 71.189\talpha: 0.051\tPLoss: -18.890\tCLoss: 4.163\tEst: 18.711\n",
      "Episode 2700\tAvg: 18.358\tMin: 3.465\tMax: 39.600\talpha: 0.052\tPLoss: -19.903\tCLoss: 2.785\tEst: 19.844\n",
      "Episode 2750\tAvg: 17.952\tMin: 5.102\tMax: 41.578\talpha: 0.052\tPLoss: -22.417\tCLoss: 2.294\tEst: 22.397\n",
      "Episode 2800\tAvg: 19.766\tMin: 2.085\tMax: 77.564\talpha: 0.054\tPLoss: -18.626\tCLoss: 4.294\tEst: 18.695\n",
      "Episode 2850\tAvg: 21.964\tMin: 4.087\tMax: 72.672\talpha: 0.052\tPLoss: -24.073\tCLoss: 7.790\tEst: 24.164\n",
      "Episode 2900\tAvg: 23.764\tMin: 5.899\tMax: 71.543\talpha: 0.053\tPLoss: -24.199\tCLoss: 1.073\tEst: 23.948\n",
      "Episode 2950\tAvg: 25.821\tMin: 6.588\tMax: 66.573\talpha: 0.051\tPLoss: -20.290\tCLoss: 3.741\tEst: 20.297\n",
      "Episode 3000\tAvg: 24.229\tMin: 1.700\tMax: 65.038\talpha: 0.053\tPLoss: -21.081\tCLoss: 2.346\tEst: 20.939\n",
      "Episode 3050\tAvg: 18.704\tMin: 3.508\tMax: 52.379\talpha: 0.052\tPLoss: -23.285\tCLoss: 0.919\tEst: 23.162\n",
      "Episode 3100\tAvg: 17.244\tMin: 0.415\tMax: 59.286\talpha: 0.055\tPLoss: -24.278\tCLoss: 0.979\tEst: 23.882\n",
      "Episode 3150\tAvg: 21.804\tMin: 3.105\tMax: 59.647\talpha: 0.056\tPLoss: -22.803\tCLoss: 2.881\tEst: 22.558\n",
      "Episode 3200\tAvg: 27.419\tMin: 4.317\tMax: 80.877\talpha: 0.053\tPLoss: -26.275\tCLoss: 0.860\tEst: 26.041\n",
      "Episode 3250\tAvg: 26.391\tMin: 3.921\tMax: 56.728\talpha: 0.051\tPLoss: -25.315\tCLoss: 3.405\tEst: 25.264\n",
      "Episode 3300\tAvg: 23.980\tMin: 9.049\tMax: 56.077\talpha: 0.050\tPLoss: -25.180\tCLoss: 3.163\tEst: 25.042\n",
      "Episode 3350\tAvg: 26.531\tMin: 2.475\tMax: 71.077\talpha: 0.054\tPLoss: -26.304\tCLoss: 2.675\tEst: 26.324\n",
      "Episode 3400\tAvg: 27.834\tMin: 1.572\tMax: 72.635\talpha: 0.052\tPLoss: -23.694\tCLoss: 3.779\tEst: 23.708\n",
      "Episode 3450\tAvg: 28.259\tMin: 7.767\tMax: 101.156\talpha: 0.059\tPLoss: -26.204\tCLoss: 2.958\tEst: 26.361\n",
      "Episode 3500\tAvg: 30.904\tMin: 6.435\tMax: 110.931\talpha: 0.064\tPLoss: -25.778\tCLoss: 17.201\tEst: 25.762\n",
      "Episode 3550\tAvg: 31.617\tMin: 2.613\tMax: 78.781\talpha: 0.065\tPLoss: -28.020\tCLoss: 1.050\tEst: 27.733\n",
      "Episode 3600\tAvg: 32.447\tMin: 2.835\tMax: 109.788\talpha: 0.064\tPLoss: -26.249\tCLoss: 7.512\tEst: 26.010\n",
      "Episode 3650\tAvg: 30.585\tMin: 4.865\tMax: 89.192\talpha: 0.064\tPLoss: -27.892\tCLoss: 12.660\tEst: 27.890\n",
      "Episode 3700\tAvg: 31.877\tMin: 11.299\tMax: 103.982\talpha: 0.063\tPLoss: -26.415\tCLoss: 7.824\tEst: 26.090\n",
      "Episode 3750\tAvg: 31.026\tMin: 3.064\tMax: 83.112\talpha: 0.063\tPLoss: -29.987\tCLoss: 12.873\tEst: 30.106\n",
      "Episode 3800\tAvg: 31.154\tMin: 9.744\tMax: 84.012\talpha: 0.070\tPLoss: -32.991\tCLoss: 2.490\tEst: 33.027\n",
      "Episode 3850\tAvg: 36.545\tMin: 9.362\tMax: 113.058\talpha: 0.082\tPLoss: -29.926\tCLoss: 8.807\tEst: 29.650\n",
      "Episode 3900\tAvg: 38.268\tMin: 6.757\tMax: 133.564\talpha: 0.081\tPLoss: -32.935\tCLoss: 4.050\tEst: 32.662\n",
      "Episode 3950\tAvg: 36.380\tMin: 3.117\tMax: 138.403\talpha: 0.077\tPLoss: -33.268\tCLoss: 4.483\tEst: 32.988\n",
      "Episode 4000\tAvg: 36.742\tMin: 6.946\tMax: 111.243\talpha: 0.073\tPLoss: -32.896\tCLoss: 9.118\tEst: 32.599\n",
      "Episode 4050\tAvg: 38.247\tMin: 1.819\tMax: 150.639\talpha: 0.064\tPLoss: -34.150\tCLoss: 8.513\tEst: 33.825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4100\tAvg: 38.407\tMin: 6.915\tMax: 185.954\talpha: 0.065\tPLoss: -38.411\tCLoss: 1.166\tEst: 38.337\n",
      "Episode 4150\tAvg: 40.683\tMin: 5.894\tMax: 150.560\talpha: 0.066\tPLoss: -40.070\tCLoss: 5.408\tEst: 40.098\n",
      "Episode 4200\tAvg: 42.489\tMin: 8.606\tMax: 136.555\talpha: 0.067\tPLoss: -35.455\tCLoss: 15.958\tEst: 35.438\n",
      "Episode 4250\tAvg: 45.635\tMin: 7.156\tMax: 169.107\talpha: 0.064\tPLoss: -34.889\tCLoss: 6.271\tEst: 34.591\n",
      "Episode 4300\tAvg: 51.830\tMin: 4.154\tMax: 247.731\talpha: 0.068\tPLoss: -41.211\tCLoss: 3.820\tEst: 41.112\n",
      "Episode 4350\tAvg: 63.992\tMin: 5.151\tMax: 288.366\talpha: 0.077\tPLoss: -43.944\tCLoss: 8.362\tEst: 43.845\n",
      "Episode 4400\tAvg: 50.859\tMin: 2.452\tMax: 91.680\talpha: 0.068\tPLoss: -39.917\tCLoss: 46.308\tEst: 39.740\n",
      "Episode 4450\tAvg: 37.080\tMin: 4.278\tMax: 180.478\talpha: 0.068\tPLoss: -33.552\tCLoss: 20.318\tEst: 33.294\n",
      "Episode 4500\tAvg: 48.499\tMin: 9.557\tMax: 208.146\talpha: 0.065\tPLoss: -42.173\tCLoss: 19.544\tEst: 42.206\n",
      "Episode 4550\tAvg: 70.892\tMin: 11.735\tMax: 313.520\talpha: 0.065\tPLoss: -45.949\tCLoss: 3.473\tEst: 45.946\n",
      "Episode 4600\tAvg: 81.977\tMin: 2.338\tMax: 451.702\talpha: 0.066\tPLoss: -47.914\tCLoss: 2.648\tEst: 47.882\n",
      "Episode 4650\tAvg: 67.495\tMin: 8.797\tMax: 200.233\talpha: 0.067\tPLoss: -42.033\tCLoss: 9.772\tEst: 42.123\n",
      "Episode 4700\tAvg: 57.850\tMin: 13.529\tMax: 201.294\talpha: 0.082\tPLoss: -42.475\tCLoss: 7.304\tEst: 42.052\n",
      "Episode 4750\tAvg: 56.980\tMin: 1.984\tMax: 348.258\talpha: 0.072\tPLoss: -39.574\tCLoss: 4.855\tEst: 39.296\n",
      "Episode 4800\tAvg: 52.684\tMin: 4.546\tMax: 236.977\talpha: 0.064\tPLoss: -36.187\tCLoss: 10.412\tEst: 36.350\n",
      "Episode 4850\tAvg: 44.490\tMin: 0.123\tMax: 207.656\talpha: 0.062\tPLoss: -27.806\tCLoss: 7.473\tEst: 27.680\n",
      "Episode 4900\tAvg: 43.090\tMin: 2.830\tMax: 128.023\talpha: 0.058\tPLoss: -31.068\tCLoss: 1.864\tEst: 31.051\n",
      "Episode 4950\tAvg: 52.160\tMin: 6.679\tMax: 201.335\talpha: 0.054\tPLoss: -35.583\tCLoss: 6.882\tEst: 35.665\n",
      "Episode 5000\tAvg: 61.376\tMin: 4.754\tMax: 347.464\talpha: 0.057\tPLoss: -36.856\tCLoss: 2.098\tEst: 36.761\n",
      "Episode 5050\tAvg: 56.725\tMin: 2.838\tMax: 175.522\talpha: 0.059\tPLoss: -34.583\tCLoss: 3.290\tEst: 34.713\n",
      "Episode 5100\tAvg: 45.650\tMin: 1.584\tMax: 103.885\talpha: 0.064\tPLoss: -37.029\tCLoss: 1.327\tEst: 36.985\n",
      "Episode 5150\tAvg: 52.093\tMin: 0.234\tMax: 193.922\talpha: 0.057\tPLoss: -25.998\tCLoss: 9.795\tEst: 26.077\n",
      "Episode 5200\tAvg: 60.830\tMin: 4.459\tMax: 179.429\talpha: 0.054\tPLoss: -38.294\tCLoss: 4.384\tEst: 38.152\n",
      "Episode 5250\tAvg: 68.167\tMin: 4.998\tMax: 254.719\talpha: 0.058\tPLoss: -44.485\tCLoss: 2.057\tEst: 44.351\n",
      "Episode 5300\tAvg: 87.721\tMin: 6.410\tMax: 525.880\talpha: 0.056\tPLoss: -44.939\tCLoss: 2.586\tEst: 44.768\n",
      "Episode 5350\tAvg: 84.900\tMin: 10.460\tMax: 360.981\talpha: 0.063\tPLoss: -41.539\tCLoss: 15.080\tEst: 41.519\n",
      "Episode 5400\tAvg: 74.100\tMin: 16.130\tMax: 340.146\talpha: 0.069\tPLoss: -47.280\tCLoss: 0.860\tEst: 47.052\n",
      "Episode 5450\tAvg: 74.242\tMin: 10.394\tMax: 274.062\talpha: 0.064\tPLoss: -48.094\tCLoss: 10.039\tEst: 47.955\n",
      "Episode 5500\tAvg: 75.998\tMin: 2.668\tMax: 242.352\talpha: 0.064\tPLoss: -45.198\tCLoss: 9.442\tEst: 45.056\n",
      "Episode 5550\tAvg: 80.273\tMin: 7.364\tMax: 242.159\talpha: 0.069\tPLoss: -50.284\tCLoss: 3.892\tEst: 50.051\n",
      "Episode 5600\tAvg: 89.752\tMin: 9.306\tMax: 291.593\talpha: 0.066\tPLoss: -55.333\tCLoss: 5.993\tEst: 55.302\n",
      "Episode 5650\tAvg: 123.714\tMin: 9.063\tMax: 549.150\talpha: 0.066\tPLoss: -52.506\tCLoss: 6.819\tEst: 52.410\n",
      "Episode 5700\tAvg: 150.143\tMin: 7.895\tMax: 565.390\talpha: 0.071\tPLoss: -61.569\tCLoss: 2.988\tEst: 61.435\n",
      "Episode 5750\tAvg: 145.222\tMin: 12.852\tMax: 548.857\talpha: 0.073\tPLoss: -66.135\tCLoss: 16.552\tEst: 66.014\n",
      "Episode 5800\tAvg: 130.900\tMin: 8.607\tMax: 615.821\talpha: 0.078\tPLoss: -69.821\tCLoss: 5.441\tEst: 69.565\n",
      "Episode 5850\tAvg: 177.088\tMin: 2.553\tMax: 663.482\talpha: 0.066\tPLoss: -78.667\tCLoss: 11.547\tEst: 78.543\n",
      "Episode 5900\tAvg: 205.900\tMin: 20.222\tMax: 645.061\talpha: 0.069\tPLoss: -84.269\tCLoss: 5.575\tEst: 83.748\n",
      "Episode 5950\tAvg: 150.219\tMin: 4.688\tMax: 589.694\talpha: 0.078\tPLoss: -79.611\tCLoss: 2.527\tEst: 79.480\n",
      "Episode 6000\tAvg: 136.177\tMin: 10.075\tMax: 555.750\talpha: 0.078\tPLoss: -77.589\tCLoss: 23.034\tEst: 77.849\n",
      "Episode 6050\tAvg: 151.554\tMin: 6.544\tMax: 763.865\talpha: 0.091\tPLoss: -76.826\tCLoss: 2.914\tEst: 76.291\n",
      "Episode 6100\tAvg: 155.301\tMin: 10.947\tMax: 828.893\talpha: 0.080\tPLoss: -85.769\tCLoss: 5.496\tEst: 85.109\n",
      "Episode 6150\tAvg: 154.717\tMin: 6.858\tMax: 792.593\talpha: 0.076\tPLoss: -84.453\tCLoss: 0.743\tEst: 84.596\n",
      "Episode 6200\tAvg: 168.268\tMin: 6.192\tMax: 850.796\talpha: 0.076\tPLoss: -81.169\tCLoss: 6.207\tEst: 80.849\n",
      "Episode 6250\tAvg: 163.433\tMin: 7.512\tMax: 765.056\talpha: 0.070\tPLoss: -80.019\tCLoss: 6.531\tEst: 79.979\n",
      "Episode 6300\tAvg: 169.402\tMin: 6.313\tMax: 811.394\talpha: 0.075\tPLoss: -81.684\tCLoss: 4.952\tEst: 81.247\n",
      "Episode 6350\tAvg: 170.734\tMin: 2.928\tMax: 548.113\talpha: 0.084\tPLoss: -81.360\tCLoss: 263.458\tEst: 81.259\n",
      "Episode 6400\tAvg: 185.704\tMin: 10.465\tMax: 830.661\talpha: 0.078\tPLoss: -84.306\tCLoss: 5.878\tEst: 83.631\n",
      "Episode 6450\tAvg: 226.590\tMin: 6.100\tMax: 768.640\talpha: 0.076\tPLoss: -79.605\tCLoss: 5.305\tEst: 78.998\n",
      "Episode 6500\tAvg: 189.271\tMin: 5.164\tMax: 825.286\talpha: 0.092\tPLoss: -86.820\tCLoss: 21.745\tEst: 86.829\n",
      "Episode 6550\tAvg: 164.467\tMin: 3.316\tMax: 779.788\talpha: 0.080\tPLoss: -82.083\tCLoss: 5.737\tEst: 81.741\n",
      "Episode 6600\tAvg: 188.479\tMin: 4.769\tMax: 655.107\talpha: 0.075\tPLoss: -88.085\tCLoss: 2.635\tEst: 87.986\n",
      "Episode 6650\tAvg: 183.606\tMin: 10.059\tMax: 688.370\talpha: 0.073\tPLoss: -78.502\tCLoss: 16.829\tEst: 78.452\n",
      "Episode 6700\tAvg: 152.562\tMin: 16.401\tMax: 543.137\talpha: 0.072\tPLoss: -82.787\tCLoss: 12.673\tEst: 82.742\n",
      "Episode 6750\tAvg: 136.833\tMin: 3.078\tMax: 421.032\talpha: 0.080\tPLoss: -80.673\tCLoss: 21.741\tEst: 80.720\n",
      "Episode 6800\tAvg: 150.334\tMin: 11.755\tMax: 583.746\talpha: 0.086\tPLoss: -85.179\tCLoss: 3.561\tEst: 84.677\n",
      "Episode 6850\tAvg: 186.706\tMin: 3.221\tMax: 839.806\talpha: 0.081\tPLoss: -86.503\tCLoss: 2.950\tEst: 86.440\n",
      "Episode 6900\tAvg: 218.945\tMin: 27.956\tMax: 883.545\talpha: 0.075\tPLoss: -93.638\tCLoss: 2.072\tEst: 93.641\n",
      "Episode 6950\tAvg: 206.688\tMin: 3.324\tMax: 899.506\talpha: 0.090\tPLoss: -85.786\tCLoss: 5.574\tEst: 85.681\n",
      "Episode 7000\tAvg: 220.514\tMin: 0.757\tMax: 888.747\talpha: 0.083\tPLoss: -89.552\tCLoss: 2.263\tEst: 89.280\n",
      "Episode 7050\tAvg: 254.227\tMin: 9.553\tMax: 871.546\talpha: 0.083\tPLoss: -88.803\tCLoss: 5.668\tEst: 88.828\n",
      "Episode 7100\tAvg: 296.142\tMin: 2.119\tMax: 969.071\talpha: 0.073\tPLoss: -98.601\tCLoss: 5.353\tEst: 98.146\n",
      "Episode 7150\tAvg: 256.094\tMin: 11.735\tMax: 938.193\talpha: 0.093\tPLoss: -95.804\tCLoss: 16.828\tEst: 95.596\n",
      "Episode 7200\tAvg: 218.089\tMin: 10.993\tMax: 853.190\talpha: 0.090\tPLoss: -93.964\tCLoss: 8.847\tEst: 93.984\n",
      "Episode 7250\tAvg: 241.541\tMin: 8.518\tMax: 853.661\talpha: 0.085\tPLoss: -92.546\tCLoss: 3.781\tEst: 92.013\n",
      "Episode 7300\tAvg: 196.641\tMin: 0.490\tMax: 878.235\talpha: 0.069\tPLoss: -85.643\tCLoss: 1.381\tEst: 85.583\n",
      "Episode 7350\tAvg: 252.518\tMin: 0.272\tMax: 961.383\talpha: 0.073\tPLoss: -102.012\tCLoss: 1.982\tEst: 101.799\n",
      "Episode 7400\tAvg: 310.288\tMin: 6.533\tMax: 911.370\talpha: 0.090\tPLoss: -97.497\tCLoss: 5.653\tEst: 97.359\n",
      "Episode 7450\tAvg: 231.780\tMin: 10.057\tMax: 607.442\talpha: 0.095\tPLoss: -90.770\tCLoss: 3.284\tEst: 90.377\n",
      "Episode 7500\tAvg: 227.290\tMin: 1.404\tMax: 945.528\talpha: 0.091\tPLoss: -95.443\tCLoss: 13.229\tEst: 95.849\n",
      "Episode 7550\tAvg: 267.288\tMin: 5.090\tMax: 889.098\talpha: 0.089\tPLoss: -102.674\tCLoss: 3.893\tEst: 102.319\n",
      "Episode 7600\tAvg: 282.688\tMin: 8.834\tMax: 1026.303\talpha: 0.082\tPLoss: -102.061\tCLoss: 4.887\tEst: 101.816\n",
      "Episode 7650\tAvg: 290.851\tMin: 8.131\tMax: 922.684\talpha: 0.093\tPLoss: -85.681\tCLoss: 4.948\tEst: 85.156\n",
      "Episode 7700\tAvg: 291.585\tMin: 17.394\tMax: 1049.479\talpha: 0.085\tPLoss: -101.271\tCLoss: 26.275\tEst: 101.260\n",
      "Episode 7750\tAvg: 295.860\tMin: 16.707\tMax: 909.448\talpha: 0.083\tPLoss: -88.396\tCLoss: 703.563\tEst: 88.466\n",
      "Episode 7800\tAvg: 305.050\tMin: 31.448\tMax: 1040.957\talpha: 0.080\tPLoss: -103.478\tCLoss: 5.304\tEst: 102.970\n",
      "Episode 7850\tAvg: 274.146\tMin: 7.525\tMax: 604.615\talpha: 0.071\tPLoss: -92.459\tCLoss: 3.676\tEst: 92.233\n",
      "Episode 7900\tAvg: 201.826\tMin: 14.110\tMax: 456.683\talpha: 0.074\tPLoss: -88.700\tCLoss: 5.185\tEst: 88.866\n",
      "Episode 7950\tAvg: 209.690\tMin: 14.474\tMax: 999.234\talpha: 0.082\tPLoss: -79.598\tCLoss: 10.710\tEst: 79.589\n",
      "Episode 8000\tAvg: 228.533\tMin: 18.200\tMax: 972.014\talpha: 0.088\tPLoss: -100.314\tCLoss: 3.738\tEst: 99.774\n",
      "Episode 8050\tAvg: 257.845\tMin: 11.141\tMax: 1045.874\talpha: 0.072\tPLoss: -102.515\tCLoss: 2.601\tEst: 102.144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8100\tAvg: 295.563\tMin: 8.614\tMax: 892.614\talpha: 0.069\tPLoss: -96.542\tCLoss: 2.481\tEst: 96.327\n",
      "Episode 8150\tAvg: 343.926\tMin: 20.376\tMax: 968.904\talpha: 0.069\tPLoss: -96.707\tCLoss: 3.931\tEst: 96.425\n",
      "Episode 8200\tAvg: 292.141\tMin: 5.415\tMax: 840.246\talpha: 0.088\tPLoss: -89.400\tCLoss: 30.407\tEst: 89.546\n",
      "Episode 8250\tAvg: 220.093\tMin: 12.348\tMax: 864.883\talpha: 0.080\tPLoss: -95.144\tCLoss: 47.435\tEst: 94.957\n",
      "Episode 8300\tAvg: 316.114\tMin: 5.655\tMax: 997.563\talpha: 0.074\tPLoss: -102.428\tCLoss: 2.923\tEst: 102.086\n",
      "Episode 8350\tAvg: 364.575\tMin: 20.314\tMax: 1038.500\talpha: 0.068\tPLoss: -101.441\tCLoss: 1.522\tEst: 101.643\n",
      "Episode 8400\tAvg: 323.291\tMin: 29.724\tMax: 917.779\talpha: 0.071\tPLoss: -97.730\tCLoss: 2.085\tEst: 97.851\n",
      "Episode 8450\tAvg: 322.792\tMin: 38.377\tMax: 950.004\talpha: 0.074\tPLoss: -94.466\tCLoss: 38.056\tEst: 94.328\n",
      "Episode 8500\tAvg: 357.610\tMin: 4.117\tMax: 1038.434\talpha: 0.062\tPLoss: -103.073\tCLoss: 1.490\tEst: 103.054\n",
      "Episode 8550\tAvg: 357.860\tMin: 3.765\tMax: 1051.876\talpha: 0.071\tPLoss: -106.686\tCLoss: 10.114\tEst: 106.565\n",
      "Episode 8600\tAvg: 341.698\tMin: 5.233\tMax: 1011.458\talpha: 0.073\tPLoss: -99.738\tCLoss: 4.681\tEst: 99.792\n",
      "Episode 8650\tAvg: 341.043\tMin: 8.260\tMax: 1051.072\talpha: 0.079\tPLoss: -114.652\tCLoss: 3.729\tEst: 114.391\n",
      "Episode 8700\tAvg: 369.334\tMin: 32.305\tMax: 1069.443\talpha: 0.075\tPLoss: -108.571\tCLoss: 3.555\tEst: 108.225\n",
      "Episode 8750\tAvg: 420.020\tMin: 19.970\tMax: 1025.499\talpha: 0.076\tPLoss: -100.412\tCLoss: 1.759\tEst: 99.806\n",
      "Episode 8800\tAvg: 379.872\tMin: 10.407\tMax: 1022.807\talpha: 0.074\tPLoss: -97.126\tCLoss: 19.417\tEst: 96.850\n",
      "Episode 8850\tAvg: 307.316\tMin: 6.746\tMax: 930.002\talpha: 0.081\tPLoss: -102.816\tCLoss: 4.278\tEst: 102.454\n",
      "Episode 8900\tAvg: 311.026\tMin: 2.714\tMax: 1020.711\talpha: 0.088\tPLoss: -101.983\tCLoss: 2.422\tEst: 101.620\n",
      "Episode 8950\tAvg: 322.391\tMin: 0.482\tMax: 998.021\talpha: 0.077\tPLoss: -89.065\tCLoss: 2.495\tEst: 88.897\n",
      "Episode 9000\tAvg: 316.414\tMin: 0.662\tMax: 924.982\talpha: 0.077\tPLoss: -97.933\tCLoss: 16.775\tEst: 98.040\n",
      "Episode 9050\tAvg: 312.133\tMin: 1.340\tMax: 930.839\talpha: 0.073\tPLoss: -108.324\tCLoss: 3.355\tEst: 108.418\n",
      "Episode 9100\tAvg: 333.425\tMin: 7.045\tMax: 1033.333\talpha: 0.079\tPLoss: -117.486\tCLoss: 1.074\tEst: 117.073\n",
      "Episode 9150\tAvg: 351.447\tMin: 6.484\tMax: 1128.430\talpha: 0.083\tPLoss: -115.412\tCLoss: 25.375\tEst: 114.726\n",
      "Episode 9200\tAvg: 310.281\tMin: 4.976\tMax: 1071.685\talpha: 0.079\tPLoss: -114.803\tCLoss: 2.293\tEst: 114.561\n",
      "Episode 9250\tAvg: 271.379\tMin: 22.582\tMax: 956.025\talpha: 0.100\tPLoss: -104.586\tCLoss: 7.515\tEst: 104.098\n",
      "Episode 9300\tAvg: 299.099\tMin: 11.275\tMax: 943.091\talpha: 0.074\tPLoss: -103.149\tCLoss: 353.159\tEst: 103.113\n",
      "Episode 9350\tAvg: 328.937\tMin: 5.472\tMax: 1078.905\talpha: 0.078\tPLoss: -107.500\tCLoss: 12.892\tEst: 107.544\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-5d118c224e8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# loaded.network.log_alpha = torch.nn.Parameter(torch.tensor(-0.5, dtype=torch.float32))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_to_learn_at\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplot_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-f06d7043b459>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(agent, scores, n_episodes, train_mode, episode_start, start_to_learn_at)\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# add only to memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m          \u001b[1;31m# learn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_states\u001b[0m                                                  \u001b[1;31m# roll over states to next time step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-crawl\\agents_maddpg\\sac.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mt_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUPDATE_LOOP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                 \u001b[0mt_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_step\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRANSFER_EVERY\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt_step\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-crawl\\agents_maddpg\\sac.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritics_losses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\MyDev\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\MyDev\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loaded.network.log_alpha = torch.nn.Parameter(torch.tensor(-0.5, dtype=torch.float32))\n",
    "scores = train(loaded, scores, n_episodes=60000,train_mode=True, start_to_learn_at=0)\n",
    "plot_result(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do next\n",
    "1. From OpenAI\n",
    "    1. At test time, to see how well the policy exploits what it has learned, remove stochasticity and use the mean action instead of a sample from the distribution. This tends to improve performance over the original stochastic policy.\n",
    "    1. Explore randomly prior to start SAC befor n_steps\n",
    "    1. Use Value Network\n",
    "    1. Set proper entropy based on formula from OpenAI H = - log(x)\n",
    "\n",
    "## To experiment\n",
    "1. Set entropy target as -np.prod(self.env.action_space.shape).item()  # heuristic value from Tuomas according to rlkit\n",
    "1. Use Prioritized Experience Replay\n",
    "1. Set fixed alpha\n",
    "1. What about using that approximator of tanh likelihood trick in PPO Model?\n",
    "1. What about using target actor as well?\n",
    "1. What about using MADDPG with alpha optimization, entropy, and with the approximator of tanh likelihood\n",
    "1. What about clipped double-Q trick on PPO and DDPG?\n",
    "\n",
    "\n",
    "OpenAI\n",
    "1. https://spinningup.openai.com/en/latest/algorithms/sac.html#pseudocode\n",
    "\n",
    "1. Use Value network local and target instead of Q Network local and Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving the agent a vision of what will happen in the future should help. But we have seen that a high bootstrap size at early training will make the agent failing to get enough data. However, at this stage, the agent is able to move a bit so gradually increasing the bootstrap might help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9400\tAvg: 410.832\tMin: 0.670\tMax: 1221.947\talpha: 0.058\tPLoss: -30.175\tCLoss: 0.239\tEst: 29.878\n",
      "Episode 9450\tAvg: 254.566\tMin: -0.327\tMax: 39.428\talpha: 0.055\tPLoss: -50.425\tCLoss: 3.992\tEst: 50.403\n",
      "Episode 9500\tAvg: 9.475\tMin: -0.799\tMax: 24.305\talpha: 0.054\tPLoss: -53.621\tCLoss: 5.634\tEst: 53.455\n",
      "Episode 9550\tAvg: 5.134\tMin: -4.915\tMax: 15.599\talpha: 0.049\tPLoss: -57.712\tCLoss: 106.740\tEst: 57.351\n",
      "Episode 9600\tAvg: 2.346\tMin: -3.498\tMax: 7.154\talpha: 0.049\tPLoss: -50.163\tCLoss: 2.999\tEst: 50.227\n",
      "Episode 9650\tAvg: 6.674\tMin: -3.908\tMax: 50.557\talpha: 0.045\tPLoss: -54.673\tCLoss: 6.979\tEst: 54.738\n",
      "Episode 9700\tAvg: 12.164\tMin: 0.845\tMax: 53.947\talpha: 0.054\tPLoss: -49.331\tCLoss: 8.921\tEst: 49.678\n",
      "Episode 9750\tAvg: 11.711\tMin: -3.054\tMax: 35.900\talpha: 0.058\tPLoss: -41.747\tCLoss: 6.393\tEst: 41.286\n",
      "Episode 9800\tAvg: 9.714\tMin: 0.132\tMax: 19.789\talpha: 0.055\tPLoss: -32.320\tCLoss: 5.177\tEst: 31.945\n",
      "Episode 9850\tAvg: 9.209\tMin: 4.024\tMax: 16.471\talpha: 0.053\tPLoss: -23.405\tCLoss: 9.554\tEst: 23.231\n",
      "Episode 9900\tAvg: 10.368\tMin: 4.412\tMax: 27.561\talpha: 0.054\tPLoss: -21.714\tCLoss: 3.850\tEst: 21.337\n",
      "Episode 9950\tAvg: 16.104\tMin: 3.097\tMax: 103.094\talpha: 0.051\tPLoss: -16.453\tCLoss: 4.189\tEst: 16.293\n",
      "Episode 10000\tAvg: 20.726\tMin: 2.233\tMax: 68.222\talpha: 0.046\tPLoss: -18.620\tCLoss: 1.613\tEst: 18.381\n",
      "Episode 10050\tAvg: 20.425\tMin: 1.682\tMax: 44.459\talpha: 0.037\tPLoss: -20.411\tCLoss: 0.770\tEst: 20.404\n",
      "Episode 10100\tAvg: 19.095\tMin: 2.297\tMax: 43.537\talpha: 0.031\tPLoss: -23.636\tCLoss: 1.140\tEst: 23.648\n",
      "Episode 10150\tAvg: 16.606\tMin: 2.005\tMax: 43.328\talpha: 0.029\tPLoss: -23.023\tCLoss: 1.673\tEst: 23.077\n",
      "Episode 10200\tAvg: 16.086\tMin: 7.255\tMax: 47.671\talpha: 0.028\tPLoss: -23.078\tCLoss: 3.016\tEst: 23.031\n",
      "Episode 10250\tAvg: 18.503\tMin: 2.172\tMax: 70.242\talpha: 0.028\tPLoss: -23.845\tCLoss: 0.707\tEst: 23.947\n",
      "Episode 10300\tAvg: 17.075\tMin: 1.399\tMax: 31.471\talpha: 0.029\tPLoss: -22.209\tCLoss: 0.956\tEst: 22.133\n",
      "Episode 10350\tAvg: 19.890\tMin: 2.594\tMax: 66.559\talpha: 0.028\tPLoss: -20.138\tCLoss: 3.100\tEst: 20.131\n",
      "Episode 10400\tAvg: 21.937\tMin: 2.809\tMax: 42.557\talpha: 0.029\tPLoss: -20.381\tCLoss: 5.658\tEst: 20.393\n",
      "Episode 10450\tAvg: 20.231\tMin: 3.522\tMax: 45.429\talpha: 0.030\tPLoss: -22.093\tCLoss: 2.083\tEst: 22.065\n",
      "Episode 10500\tAvg: 22.589\tMin: 2.621\tMax: 72.224\talpha: 0.030\tPLoss: -22.511\tCLoss: 1.254\tEst: 22.352\n",
      "Episode 10550\tAvg: 21.873\tMin: 2.994\tMax: 52.177\talpha: 0.031\tPLoss: -23.192\tCLoss: 7.007\tEst: 23.331\n",
      "Episode 10600\tAvg: 22.917\tMin: 3.965\tMax: 76.115\talpha: 0.033\tPLoss: -23.863\tCLoss: 1.836\tEst: 23.774\n",
      "Episode 10650\tAvg: 26.299\tMin: 4.239\tMax: 63.745\talpha: 0.036\tPLoss: -22.880\tCLoss: 3.934\tEst: 22.975\n",
      "Episode 10700\tAvg: 27.648\tMin: 4.635\tMax: 66.970\talpha: 0.038\tPLoss: -21.924\tCLoss: 1.305\tEst: 21.910\n",
      "Episode 10750\tAvg: 29.930\tMin: 4.744\tMax: 77.013\talpha: 0.044\tPLoss: -23.736\tCLoss: 1.238\tEst: 23.715\n",
      "Episode 10800\tAvg: 31.732\tMin: 4.725\tMax: 90.498\talpha: 0.048\tPLoss: -28.622\tCLoss: 1.927\tEst: 28.573\n",
      "Episode 10850\tAvg: 29.761\tMin: 4.991\tMax: 67.092\talpha: 0.050\tPLoss: -26.390\tCLoss: 1.350\tEst: 26.384\n",
      "Episode 10900\tAvg: 28.429\tMin: 7.022\tMax: 82.286\talpha: 0.052\tPLoss: -30.712\tCLoss: 2.301\tEst: 30.649\n",
      "Episode 10950\tAvg: 29.623\tMin: 3.135\tMax: 133.439\talpha: 0.055\tPLoss: -30.031\tCLoss: 1.958\tEst: 30.021\n",
      "Episode 11000\tAvg: 35.208\tMin: 5.492\tMax: 107.469\talpha: 0.054\tPLoss: -30.899\tCLoss: 3.192\tEst: 30.952\n",
      "Episode 11050\tAvg: 37.738\tMin: 4.625\tMax: 148.429\talpha: 0.058\tPLoss: -32.981\tCLoss: 3.557\tEst: 32.550\n",
      "Episode 11100\tAvg: 38.756\tMin: 5.668\tMax: 105.950\talpha: 0.059\tPLoss: -36.110\tCLoss: 1.811\tEst: 35.792\n",
      "Episode 11150\tAvg: 39.161\tMin: 9.397\tMax: 102.695\talpha: 0.063\tPLoss: -35.908\tCLoss: 3.139\tEst: 35.863\n",
      "Episode 11200\tAvg: 39.932\tMin: 9.783\tMax: 141.311\talpha: 0.065\tPLoss: -37.836\tCLoss: 2.918\tEst: 37.827\n",
      "Episode 11250\tAvg: 43.284\tMin: 7.508\tMax: 89.114\talpha: 0.065\tPLoss: -34.130\tCLoss: 4.807\tEst: 34.102\n",
      "Episode 11300\tAvg: 45.427\tMin: 8.045\tMax: 109.227\talpha: 0.069\tPLoss: -40.857\tCLoss: 84.636\tEst: 40.498\n",
      "Episode 11350\tAvg: 44.007\tMin: 3.199\tMax: 124.979\talpha: 0.071\tPLoss: -41.447\tCLoss: 4.263\tEst: 41.383\n",
      "Episode 11400\tAvg: 44.903\tMin: 7.557\tMax: 216.639\talpha: 0.076\tPLoss: -43.214\tCLoss: 8.096\tEst: 43.061\n",
      "Episode 11450\tAvg: 53.689\tMin: 8.098\tMax: 264.498\talpha: 0.073\tPLoss: -41.837\tCLoss: 11.646\tEst: 42.016\n",
      "Episode 11500\tAvg: 59.021\tMin: 2.709\tMax: 302.785\talpha: 0.073\tPLoss: -50.284\tCLoss: 7.362\tEst: 49.914\n",
      "Episode 11550\tAvg: 62.778\tMin: 9.252\tMax: 236.225\talpha: 0.082\tPLoss: -47.684\tCLoss: 7.671\tEst: 46.811\n",
      "Episode 11600\tAvg: 62.276\tMin: 8.209\tMax: 178.423\talpha: 0.086\tPLoss: -44.318\tCLoss: 39.258\tEst: 44.240\n",
      "Episode 11650\tAvg: 59.385\tMin: 4.119\tMax: 246.614\talpha: 0.081\tPLoss: -53.466\tCLoss: 2.664\tEst: 53.356\n",
      "Episode 11700\tAvg: 56.736\tMin: 6.843\tMax: 175.021\talpha: 0.079\tPLoss: -57.933\tCLoss: 2.471\tEst: 57.754\n",
      "Episode 11750\tAvg: 68.441\tMin: 10.047\tMax: 704.862\talpha: 0.080\tPLoss: -57.476\tCLoss: 4.034\tEst: 57.283\n",
      "Episode 11800\tAvg: 74.152\tMin: 10.808\tMax: 270.749\talpha: 0.076\tPLoss: -58.357\tCLoss: 3.882\tEst: 57.660\n",
      "Episode 11850\tAvg: 56.858\tMin: 5.351\tMax: 221.168\talpha: 0.079\tPLoss: -54.534\tCLoss: 6.203\tEst: 54.580\n",
      "Episode 11900\tAvg: 61.824\tMin: 10.557\tMax: 265.950\talpha: 0.081\tPLoss: -54.068\tCLoss: 19.653\tEst: 53.805\n",
      "Episode 11950\tAvg: 65.310\tMin: 2.252\tMax: 219.308\talpha: 0.087\tPLoss: -59.282\tCLoss: 13.430\tEst: 59.327\n",
      "Episode 12000\tAvg: 72.872\tMin: 5.478\tMax: 352.917\talpha: 0.082\tPLoss: -56.888\tCLoss: 4.002\tEst: 56.528\n",
      "Episode 12050\tAvg: 105.083\tMin: 12.138\tMax: 449.917\talpha: 0.075\tPLoss: -66.930\tCLoss: 5.211\tEst: 66.961\n",
      "Episode 12100\tAvg: 82.139\tMin: 4.775\tMax: 244.440\talpha: 0.081\tPLoss: -62.406\tCLoss: 12.829\tEst: 62.628\n",
      "Episode 12150\tAvg: 48.573\tMin: 2.694\tMax: 231.655\talpha: 0.082\tPLoss: -55.209\tCLoss: 25.467\tEst: 55.086\n",
      "Episode 12200\tAvg: 60.899\tMin: 5.124\tMax: 275.185\talpha: 0.078\tPLoss: -57.814\tCLoss: 10.736\tEst: 57.859\n",
      "Episode 12250\tAvg: 78.887\tMin: 5.096\tMax: 358.916\talpha: 0.082\tPLoss: -68.566\tCLoss: 2.985\tEst: 68.541\n",
      "Episode 12300\tAvg: 80.833\tMin: 3.097\tMax: 306.656\talpha: 0.079\tPLoss: -66.230\tCLoss: 4.029\tEst: 66.111\n",
      "Episode 12350\tAvg: 73.044\tMin: 7.556\tMax: 339.411\talpha: 0.086\tPLoss: -67.691\tCLoss: 17.156\tEst: 67.743\n",
      "Episode 12400\tAvg: 82.411\tMin: 9.292\tMax: 317.370\talpha: 0.084\tPLoss: -62.022\tCLoss: 5.869\tEst: 61.661\n",
      "Episode 12450\tAvg: 109.465\tMin: 1.598\tMax: 750.242\talpha: 0.072\tPLoss: -57.727\tCLoss: 4.574\tEst: 57.450\n",
      "Episode 12500\tAvg: 108.254\tMin: 1.795\tMax: 352.341\talpha: 0.077\tPLoss: -63.768\tCLoss: 23.943\tEst: 63.500\n",
      "Episode 12550\tAvg: 89.615\tMin: 4.809\tMax: 332.352\talpha: 0.080\tPLoss: -62.092\tCLoss: 10.822\tEst: 61.679\n",
      "Episode 12600\tAvg: 93.383\tMin: 9.004\tMax: 344.626\talpha: 0.084\tPLoss: -64.368\tCLoss: 8.633\tEst: 63.837\n",
      "Episode 12650\tAvg: 108.151\tMin: 3.131\tMax: 555.566\talpha: 0.073\tPLoss: -69.998\tCLoss: 8.328\tEst: 69.757\n",
      "Episode 12700\tAvg: 108.863\tMin: 2.811\tMax: 463.053\talpha: 0.079\tPLoss: -71.998\tCLoss: 2.391\tEst: 71.914\n",
      "Episode 12750\tAvg: 124.259\tMin: 5.474\tMax: 695.304\talpha: 0.065\tPLoss: -76.516\tCLoss: 2.197\tEst: 76.598\n",
      "Episode 12800\tAvg: 122.540\tMin: 4.581\tMax: 406.040\talpha: 0.069\tPLoss: -71.505\tCLoss: 13.822\tEst: 71.574\n",
      "Episode 12850\tAvg: 92.834\tMin: 8.778\tMax: 444.830\talpha: 0.072\tPLoss: -77.268\tCLoss: 1.680\tEst: 77.569\n",
      "Episode 12900\tAvg: 115.050\tMin: 4.222\tMax: 646.694\talpha: 0.068\tPLoss: -75.043\tCLoss: 12.292\tEst: 74.761\n",
      "Episode 12950\tAvg: 142.755\tMin: 4.597\tMax: 432.872\talpha: 0.058\tPLoss: -79.613\tCLoss: 5.722\tEst: 79.284\n",
      "Episode 13000\tAvg: 127.759\tMin: 10.773\tMax: 360.607\talpha: 0.061\tPLoss: -75.833\tCLoss: 14.398\tEst: 75.912\n",
      "Episode 13050\tAvg: 123.334\tMin: 16.494\tMax: 593.998\talpha: 0.069\tPLoss: -73.188\tCLoss: 3.143\tEst: 73.017\n",
      "Episode 13100\tAvg: 146.287\tMin: 15.224\tMax: 490.678\talpha: 0.064\tPLoss: -77.423\tCLoss: 2.795\tEst: 77.177\n",
      "Episode 13150\tAvg: 123.749\tMin: 0.581\tMax: 855.003\talpha: 0.072\tPLoss: -76.435\tCLoss: 8.430\tEst: 75.840\n",
      "Episode 13200\tAvg: 123.987\tMin: 12.398\tMax: 848.173\talpha: 0.083\tPLoss: -80.355\tCLoss: 3.944\tEst: 80.103\n",
      "Episode 13250\tAvg: 143.279\tMin: 10.136\tMax: 643.168\talpha: 0.076\tPLoss: -79.918\tCLoss: 7.324\tEst: 79.599\n",
      "Episode 13300\tAvg: 132.348\tMin: 2.608\tMax: 669.540\talpha: 0.066\tPLoss: -80.499\tCLoss: 4.193\tEst: 79.783\n",
      "Episode 13350\tAvg: 129.504\tMin: 3.883\tMax: 720.280\talpha: 0.078\tPLoss: -76.781\tCLoss: 7.542\tEst: 76.531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13400\tAvg: 138.018\tMin: 8.536\tMax: 358.630\talpha: 0.071\tPLoss: -79.283\tCLoss: 35.009\tEst: 79.105\n",
      "Episode 13450\tAvg: 115.744\tMin: 8.180\tMax: 295.568\talpha: 0.071\tPLoss: -76.243\tCLoss: 7.693\tEst: 76.099\n",
      "Episode 13500\tAvg: 88.205\tMin: 7.116\tMax: 274.749\talpha: 0.076\tPLoss: -72.092\tCLoss: 4.538\tEst: 71.766\n",
      "Episode 13550\tAvg: 102.060\tMin: 7.746\tMax: 462.206\talpha: 0.069\tPLoss: -75.891\tCLoss: 6.149\tEst: 75.848\n",
      "Episode 13600\tAvg: 140.766\tMin: 2.976\tMax: 836.732\talpha: 0.061\tPLoss: -80.308\tCLoss: 8.908\tEst: 80.092\n",
      "Episode 13650\tAvg: 172.777\tMin: 10.109\tMax: 675.530\talpha: 0.061\tPLoss: -80.981\tCLoss: 1.127\tEst: 80.614\n",
      "Episode 13700\tAvg: 155.438\tMin: 12.400\tMax: 604.810\talpha: 0.063\tPLoss: -84.736\tCLoss: 1.358\tEst: 84.667\n",
      "Episode 13750\tAvg: 143.163\tMin: 22.552\tMax: 466.370\talpha: 0.075\tPLoss: -85.620\tCLoss: 10.142\tEst: 84.976\n",
      "Episode 13800\tAvg: 143.367\tMin: 11.878\tMax: 873.261\talpha: 0.075\tPLoss: -81.335\tCLoss: 3.686\tEst: 81.261\n",
      "Episode 13850\tAvg: 128.440\tMin: 0.959\tMax: 488.869\talpha: 0.082\tPLoss: -73.867\tCLoss: 11.675\tEst: 73.713\n",
      "Episode 13900\tAvg: 120.125\tMin: 8.509\tMax: 692.566\talpha: 0.080\tPLoss: -70.624\tCLoss: 2.933\tEst: 70.462\n",
      "Episode 13950\tAvg: 110.056\tMin: 1.787\tMax: 423.611\talpha: 0.087\tPLoss: -75.728\tCLoss: 5.211\tEst: 75.831\n",
      "Episode 14000\tAvg: 107.984\tMin: 5.876\tMax: 584.515\talpha: 0.081\tPLoss: -76.873\tCLoss: 6.649\tEst: 76.935\n",
      "Episode 14050\tAvg: 142.556\tMin: 4.553\tMax: 600.413\talpha: 0.062\tPLoss: -88.378\tCLoss: 0.697\tEst: 88.542\n",
      "Episode 14100\tAvg: 164.140\tMin: 15.641\tMax: 825.283\talpha: 0.069\tPLoss: -95.050\tCLoss: 2.076\tEst: 95.139\n",
      "Episode 14150\tAvg: 182.711\tMin: 10.298\tMax: 1012.561\talpha: 0.084\tPLoss: -95.203\tCLoss: 3.049\tEst: 94.967\n",
      "Episode 14200\tAvg: 191.655\tMin: 2.992\tMax: 848.840\talpha: 0.087\tPLoss: -89.905\tCLoss: 12.455\tEst: 89.674\n",
      "Episode 14250\tAvg: 155.862\tMin: 7.947\tMax: 516.055\talpha: 0.075\tPLoss: -90.893\tCLoss: 3.718\tEst: 90.756\n",
      "Episode 14300\tAvg: 123.634\tMin: 7.705\tMax: 486.231\talpha: 0.086\tPLoss: -86.153\tCLoss: 8.330\tEst: 85.970\n",
      "Episode 14350\tAvg: 142.109\tMin: -1.527\tMax: 785.135\talpha: 0.075\tPLoss: -91.323\tCLoss: 1.875\tEst: 91.534\n",
      "Episode 14400\tAvg: 190.810\tMin: 16.705\tMax: 952.029\talpha: 0.084\tPLoss: -88.803\tCLoss: 13.097\tEst: 88.409\n",
      "Episode 14450\tAvg: 194.897\tMin: 6.857\tMax: 977.447\talpha: 0.072\tPLoss: -96.734\tCLoss: 33.340\tEst: 96.720\n",
      "Episode 14500\tAvg: 167.894\tMin: 18.159\tMax: 662.352\talpha: 0.072\tPLoss: -91.725\tCLoss: 11.799\tEst: 91.424\n",
      "Episode 14550\tAvg: 176.267\tMin: 8.994\tMax: 911.496\talpha: 0.067\tPLoss: -92.066\tCLoss: 1.771\tEst: 91.720\n",
      "Episode 14600\tAvg: 178.230\tMin: 10.587\tMax: 923.219\talpha: 0.072\tPLoss: -90.169\tCLoss: 47.907\tEst: 90.268\n",
      "Episode 14650\tAvg: 147.972\tMin: 4.728\tMax: 916.425\talpha: 0.080\tPLoss: -81.792\tCLoss: 30.934\tEst: 81.536\n",
      "Episode 14700\tAvg: 167.359\tMin: 6.280\tMax: 648.901\talpha: 0.076\tPLoss: -92.897\tCLoss: 5.288\tEst: 92.576\n",
      "Episode 14750\tAvg: 169.231\tMin: 15.109\tMax: 686.927\talpha: 0.088\tPLoss: -102.189\tCLoss: 4.532\tEst: 102.005\n",
      "Episode 14800\tAvg: 144.786\tMin: 4.243\tMax: 641.158\talpha: 0.090\tPLoss: -87.384\tCLoss: 17.327\tEst: 86.793\n",
      "Episode 14850\tAvg: 175.618\tMin: 16.768\tMax: 894.867\talpha: 0.073\tPLoss: -87.720\tCLoss: 16.036\tEst: 87.941\n",
      "Episode 14900\tAvg: 192.720\tMin: 13.103\tMax: 1104.414\talpha: 0.076\tPLoss: -103.563\tCLoss: 13.967\tEst: 103.599\n",
      "Episode 14950\tAvg: 144.351\tMin: 4.536\tMax: 467.034\talpha: 0.081\tPLoss: -96.949\tCLoss: 8.287\tEst: 96.560\n",
      "Episode 15000\tAvg: 139.445\tMin: 5.681\tMax: 1025.891\talpha: 0.081\tPLoss: -94.810\tCLoss: 33.811\tEst: 94.351\n",
      "Episode 15050\tAvg: 166.199\tMin: 6.174\tMax: 936.734\talpha: 0.077\tPLoss: -90.272\tCLoss: 10.987\tEst: 90.302\n",
      "Episode 15100\tAvg: 165.691\tMin: 1.814\tMax: 822.406\talpha: 0.072\tPLoss: -94.732\tCLoss: 8.302\tEst: 94.712\n",
      "Episode 15150\tAvg: 153.285\tMin: 6.619\tMax: 557.549\talpha: 0.080\tPLoss: -93.347\tCLoss: 4.918\tEst: 93.250\n",
      "Episode 15200\tAvg: 167.386\tMin: 17.926\tMax: 1025.068\talpha: 0.082\tPLoss: -93.859\tCLoss: 5.819\tEst: 93.801\n",
      "Episode 15250\tAvg: 205.696\tMin: 10.228\tMax: 1189.020\talpha: 0.087\tPLoss: -103.815\tCLoss: 6.326\tEst: 103.932\n",
      "Episode 15300\tAvg: 194.003\tMin: 7.086\tMax: 1077.954\talpha: 0.083\tPLoss: -92.577\tCLoss: 69.890\tEst: 92.774\n",
      "Episode 15350\tAvg: 161.262\tMin: 9.223\tMax: 791.619\talpha: 0.091\tPLoss: -100.146\tCLoss: 49.985\tEst: 99.916\n",
      "Episode 15400\tAvg: 158.533\tMin: 3.480\tMax: 996.507\talpha: 0.077\tPLoss: -95.910\tCLoss: 10.693\tEst: 95.167\n",
      "Episode 15450\tAvg: 173.556\tMin: 0.772\tMax: 658.763\talpha: 0.091\tPLoss: -99.453\tCLoss: 2.184\tEst: 99.525\n",
      "Episode 15500\tAvg: 198.228\tMin: 15.449\tMax: 878.686\talpha: 0.078\tPLoss: -101.927\tCLoss: 15.285\tEst: 101.712\n",
      "Episode 15550\tAvg: 196.113\tMin: 14.041\tMax: 688.751\talpha: 0.069\tPLoss: -102.952\tCLoss: 53.941\tEst: 102.767\n",
      "Episode 15600\tAvg: 172.181\tMin: 9.154\tMax: 845.983\talpha: 0.081\tPLoss: -100.738\tCLoss: 33.625\tEst: 100.624\n",
      "Episode 15650\tAvg: 214.275\tMin: 7.086\tMax: 1062.605\talpha: 0.083\tPLoss: -111.746\tCLoss: 37.967\tEst: 111.595\n",
      "Episode 15700\tAvg: 226.648\tMin: 5.655\tMax: 825.227\talpha: 0.079\tPLoss: -116.090\tCLoss: 5.502\tEst: 115.665\n",
      "Episode 15750\tAvg: 173.876\tMin: 6.103\tMax: 479.459\talpha: 0.083\tPLoss: -106.969\tCLoss: 9.543\tEst: 107.291\n",
      "Episode 15800\tAvg: 143.805\tMin: 13.823\tMax: 544.796\talpha: 0.091\tPLoss: -112.029\tCLoss: 7.756\tEst: 111.566\n",
      "Episode 15850\tAvg: 164.640\tMin: 7.472\tMax: 760.890\talpha: 0.081\tPLoss: -111.161\tCLoss: 2.210\tEst: 111.052\n",
      "Episode 15900\tAvg: 179.398\tMin: 15.564\tMax: 892.769\talpha: 0.098\tPLoss: -107.873\tCLoss: 6.559\tEst: 108.012\n",
      "Episode 15950\tAvg: 158.139\tMin: 2.314\tMax: 538.497\talpha: 0.092\tPLoss: -98.220\tCLoss: 53.202\tEst: 98.171\n",
      "Episode 16000\tAvg: 149.727\tMin: 6.858\tMax: 660.703\talpha: 0.081\tPLoss: -98.503\tCLoss: 7.075\tEst: 97.986\n",
      "Episode 16050\tAvg: 158.611\tMin: 3.735\tMax: 662.622\talpha: 0.085\tPLoss: -104.671\tCLoss: 127.467\tEst: 104.893\n",
      "Episode 16100\tAvg: 205.868\tMin: 6.673\tMax: 856.779\talpha: 0.084\tPLoss: -111.118\tCLoss: 57.087\tEst: 110.541\n",
      "Episode 16150\tAvg: 215.212\tMin: 13.035\tMax: 644.929\talpha: 0.088\tPLoss: -82.022\tCLoss: 61.326\tEst: 81.709\n",
      "Episode 16200\tAvg: 164.157\tMin: 15.919\tMax: 460.633\talpha: 0.089\tPLoss: -104.198\tCLoss: 6.253\tEst: 103.917\n",
      "Episode 16250\tAvg: 163.544\tMin: 15.015\tMax: 556.300\talpha: 0.087\tPLoss: -104.639\tCLoss: 4.916\tEst: 104.963\n",
      "Episode 16300\tAvg: 173.278\tMin: 10.132\tMax: 962.477\talpha: 0.089\tPLoss: -102.862\tCLoss: 2.277\tEst: 102.943\n",
      "Episode 16350\tAvg: 169.961\tMin: 6.360\tMax: 568.762\talpha: 0.084\tPLoss: -104.163\tCLoss: 3.952\tEst: 104.170\n",
      "Episode 16400\tAvg: 183.157\tMin: 10.358\tMax: 745.437\talpha: 0.075\tPLoss: -100.741\tCLoss: 13.528\tEst: 100.561\n",
      "Episode 16450\tAvg: 183.815\tMin: 11.584\tMax: 829.421\talpha: 0.079\tPLoss: -106.796\tCLoss: 22.894\tEst: 106.620\n",
      "Episode 16500\tAvg: 168.281\tMin: 12.141\tMax: 476.737\talpha: 0.080\tPLoss: -106.302\tCLoss: 59.578\tEst: 106.059\n",
      "Episode 16550\tAvg: 145.404\tMin: 8.313\tMax: 449.968\talpha: 0.077\tPLoss: -94.989\tCLoss: 3.248\tEst: 94.540\n",
      "Episode 16600\tAvg: 164.896\tMin: 17.903\tMax: 744.783\talpha: 0.073\tPLoss: -97.631\tCLoss: 3.390\tEst: 97.512\n",
      "Episode 16650\tAvg: 198.944\tMin: 7.503\tMax: 730.689\talpha: 0.082\tPLoss: -109.503\tCLoss: 6.964\tEst: 109.120\n",
      "Episode 16700\tAvg: 206.088\tMin: 8.202\tMax: 718.154\talpha: 0.097\tPLoss: -109.296\tCLoss: 3.991\tEst: 108.696\n",
      "Episode 16750\tAvg: 191.770\tMin: 22.104\tMax: 620.189\talpha: 0.090\tPLoss: -109.284\tCLoss: 3.754\tEst: 109.100\n",
      "Episode 16800\tAvg: 197.558\tMin: 7.412\tMax: 815.888\talpha: 0.103\tPLoss: -109.893\tCLoss: 32.458\tEst: 109.741\n",
      "Episode 16850\tAvg: 225.730\tMin: 8.964\tMax: 772.163\talpha: 0.082\tPLoss: -117.153\tCLoss: 8.282\tEst: 116.931\n",
      "Episode 16900\tAvg: 211.864\tMin: 14.906\tMax: 712.526\talpha: 0.093\tPLoss: -115.418\tCLoss: 18.115\tEst: 114.951\n",
      "Episode 16950\tAvg: 211.652\tMin: 9.283\tMax: 736.238\talpha: 0.087\tPLoss: -109.744\tCLoss: 5.338\tEst: 109.156\n",
      "Episode 17000\tAvg: 257.477\tMin: 6.755\tMax: 988.721\talpha: 0.085\tPLoss: -115.769\tCLoss: 3.318\tEst: 115.663\n",
      "Episode 17050\tAvg: 250.408\tMin: 18.833\tMax: 766.903\talpha: 0.086\tPLoss: -112.714\tCLoss: 11.399\tEst: 111.922\n",
      "Episode 17100\tAvg: 220.760\tMin: 7.041\tMax: 1026.661\talpha: 0.093\tPLoss: -111.009\tCLoss: 32.146\tEst: 111.087\n",
      "Episode 17150\tAvg: 193.889\tMin: 18.062\tMax: 772.838\talpha: 0.091\tPLoss: -118.597\tCLoss: 9.667\tEst: 118.882\n",
      "Episode 17200\tAvg: 138.794\tMin: 10.896\tMax: 449.079\talpha: 0.096\tPLoss: -112.557\tCLoss: 8.399\tEst: 112.541\n",
      "Episode 17250\tAvg: 120.332\tMin: 16.811\tMax: 361.921\talpha: 0.099\tPLoss: -98.715\tCLoss: 6.944\tEst: 98.456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17300\tAvg: 176.276\tMin: 11.227\tMax: 860.437\talpha: 0.096\tPLoss: -100.302\tCLoss: 5.456\tEst: 100.550\n",
      "Episode 17350\tAvg: 215.956\tMin: 37.681\tMax: 930.180\talpha: 0.095\tPLoss: -113.980\tCLoss: 27.427\tEst: 113.854\n",
      "Episode 17400\tAvg: 224.305\tMin: 17.233\tMax: 1056.956\talpha: 0.103\tPLoss: -117.419\tCLoss: 3.760\tEst: 116.370\n",
      "Episode 17450\tAvg: 243.160\tMin: 19.062\tMax: 1211.840\talpha: 0.100\tPLoss: -119.690\tCLoss: 5.648\tEst: 119.123\n",
      "Episode 17500\tAvg: 252.286\tMin: 28.750\tMax: 1236.513\talpha: 0.096\tPLoss: -115.197\tCLoss: 7.390\tEst: 115.156\n",
      "Episode 17550\tAvg: 244.899\tMin: 16.049\tMax: 961.475\talpha: 0.092\tPLoss: -110.491\tCLoss: 5.196\tEst: 109.962\n",
      "Episode 17600\tAvg: 215.137\tMin: 1.151\tMax: 681.357\talpha: 0.089\tPLoss: -110.806\tCLoss: 4.232\tEst: 110.889\n",
      "Episode 17650\tAvg: 207.496\tMin: 16.218\tMax: 664.123\talpha: 0.098\tPLoss: -118.968\tCLoss: 10.112\tEst: 119.182\n",
      "Episode 17700\tAvg: 224.719\tMin: 4.929\tMax: 623.731\talpha: 0.103\tPLoss: -109.638\tCLoss: 32.974\tEst: 109.273\n",
      "Episode 17750\tAvg: 231.852\tMin: 11.770\tMax: 1155.601\talpha: 0.106\tPLoss: -114.527\tCLoss: 13.047\tEst: 114.530\n",
      "Episode 17800\tAvg: 231.804\tMin: 2.673\tMax: 1228.167\talpha: 0.107\tPLoss: -112.315\tCLoss: 26.315\tEst: 112.643\n",
      "Episode 17850\tAvg: 235.129\tMin: 7.970\tMax: 804.055\talpha: 0.104\tPLoss: -118.929\tCLoss: 4.505\tEst: 118.423\n",
      "Episode 17900\tAvg: 293.743\tMin: 47.501\tMax: 798.207\talpha: 0.101\tPLoss: -120.753\tCLoss: 2.910\tEst: 120.506\n",
      "Episode 17950\tAvg: 317.642\tMin: 18.267\tMax: 1278.954\talpha: 0.099\tPLoss: -126.917\tCLoss: 2.538\tEst: 125.722\n",
      "Episode 18000\tAvg: 268.665\tMin: 17.182\tMax: 836.529\talpha: 0.105\tPLoss: -133.067\tCLoss: 8.743\tEst: 133.260\n",
      "Episode 18050\tAvg: 272.012\tMin: 23.601\tMax: 982.654\talpha: 0.102\tPLoss: -115.897\tCLoss: 8.544\tEst: 115.703\n",
      "Episode 18100\tAvg: 272.620\tMin: 43.094\tMax: 946.578\talpha: 0.103\tPLoss: -119.133\tCLoss: 4.861\tEst: 119.089\n",
      "Episode 18150\tAvg: 262.417\tMin: 44.113\tMax: 1294.839\talpha: 0.096\tPLoss: -123.686\tCLoss: 7.502\tEst: 123.114\n",
      "Episode 18200\tAvg: 254.913\tMin: 24.171\tMax: 1282.342\talpha: 0.091\tPLoss: -122.921\tCLoss: 2.173\tEst: 122.130\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-a851657be98c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloaded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"weights/SAC/eps_9350_avg_328.937.pth\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloaded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_to_learn_at\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2e4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplot_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-f06d7043b459>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(agent, scores, n_episodes, train_mode, episode_start, start_to_learn_at)\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# add only to memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m          \u001b[1;31m# learn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_states\u001b[0m                                                  \u001b[1;31m# roll over states to next time step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-crawl\\agents_maddpg\\sac.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mt_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUPDATE_LOOP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                 \u001b[0mt_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_step\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRANSFER_EVERY\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt_step\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-crawl\\agents_maddpg\\sac.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mnew_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_log_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_with_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m         \u001b[1;31m# optimize alpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0malpha_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_alpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnew_log_probs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTARGET_ENTROPY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-crawl\\agents_maddpg\\model.py\u001b[0m in \u001b[0;36mget_with_probabilities\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_with_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_with_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mTanhGaussianActorCriticValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-crawl\\agents_maddpg\\model.py\u001b[0m in \u001b[0;36mget_with_probabilities\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mlog_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_std_linear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_std_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_std_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_std\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mstd\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0mactions_tanh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;31m# This is an approximator of the log likelihood of tanh(actions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\MyDev\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\distributions\\normal.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbroadcast_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\MyDev\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\distributions\\utils.py\u001b[0m in \u001b[0;36mbroadcast_all\u001b[1;34m(*values)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mtemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensor_idxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscalar_idxs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscalar_idxs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(6)\n",
    "torch.manual_seed(486)\n",
    "loaded, scores = Storage.load(\"weights/SAC/eps_9350_avg_328.937.pth\", device)\n",
    "loaded.memory.memory.clear()\n",
    "scores = train(loaded, scores, n_episodes=60000, train_mode=True, episode_start=len(scores)+1, start_to_learn_at=int(2e4))\n",
    "plot_result(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18250\tAvg: 332.126\tMin: 42.862\tMax: 1678.263\talpha: 0.100\tPLoss: -135.858\tCLoss: 7.311\tEst: 135.700\n",
      "Episode 18300\tAvg: 381.175\tMin: 27.181\tMax: 1060.235\talpha: 0.098\tPLoss: -134.425\tCLoss: 8.434\tEst: 134.212\n",
      "Episode 18350\tAvg: 311.476\tMin: 8.455\tMax: 1452.880\talpha: 0.092\tPLoss: -127.259\tCLoss: 11.939\tEst: 126.759\n",
      "Episode 18400\tAvg: 256.611\tMin: 16.155\tMax: 738.656\talpha: 0.091\tPLoss: -121.881\tCLoss: 29.024\tEst: 121.934\n",
      "Episode 18450\tAvg: 231.901\tMin: 27.529\tMax: 1059.038\talpha: 0.089\tPLoss: -115.977\tCLoss: 22.155\tEst: 115.386\n",
      "Episode 18500\tAvg: 264.555\tMin: 16.324\tMax: 1335.507\talpha: 0.090\tPLoss: -124.919\tCLoss: 3.045\tEst: 124.617\n",
      "Episode 18550\tAvg: 271.432\tMin: 24.845\tMax: 873.671\talpha: 0.101\tPLoss: -121.271\tCLoss: 4.413\tEst: 120.969\n",
      "Episode 18600\tAvg: 255.597\tMin: 14.721\tMax: 925.944\talpha: 0.091\tPLoss: -110.807\tCLoss: 12.170\tEst: 110.691\n",
      "Episode 18650\tAvg: 253.568\tMin: 7.019\tMax: 930.653\talpha: 0.098\tPLoss: -122.028\tCLoss: 30.531\tEst: 122.202\n",
      "Episode 18700\tAvg: 278.282\tMin: 16.533\tMax: 1348.354\talpha: 0.106\tPLoss: -122.227\tCLoss: 5.311\tEst: 121.822\n",
      "Episode 18750\tAvg: 306.001\tMin: 10.083\tMax: 959.599\talpha: 0.096\tPLoss: -114.796\tCLoss: 12.799\tEst: 114.617\n",
      "Episode 18800\tAvg: 235.284\tMin: 8.159\tMax: 688.050\talpha: 0.104\tPLoss: -112.111\tCLoss: 10.610\tEst: 112.387\n",
      "Episode 18850\tAvg: 233.342\tMin: 11.989\tMax: 1239.540\talpha: 0.097\tPLoss: -123.582\tCLoss: 7.483\tEst: 123.176\n",
      "Episode 18900\tAvg: 295.406\tMin: 8.816\tMax: 988.071\talpha: 0.094\tPLoss: -119.631\tCLoss: 16.606\tEst: 118.895\n",
      "Episode 18950\tAvg: 312.567\tMin: 6.972\tMax: 1204.924\talpha: 0.097\tPLoss: -125.681\tCLoss: 3.392\tEst: 125.350\n",
      "Episode 19000\tAvg: 333.773\tMin: 17.862\tMax: 1316.765\talpha: 0.102\tPLoss: -119.749\tCLoss: 22.394\tEst: 119.918\n",
      "Episode 19050\tAvg: 335.322\tMin: 9.386\tMax: 1201.680\talpha: 0.104\tPLoss: -128.435\tCLoss: 3.653\tEst: 128.233\n",
      "Episode 19100\tAvg: 294.928\tMin: 8.096\tMax: 1161.634\talpha: 0.095\tPLoss: -131.058\tCLoss: 0.775\tEst: 130.586\n",
      "Episode 19150\tAvg: 270.733\tMin: 18.483\tMax: 1102.870\talpha: 0.094\tPLoss: -125.218\tCLoss: 2.126\tEst: 125.204\n",
      "Episode 19200\tAvg: 359.641\tMin: 36.480\tMax: 1462.084\talpha: 0.094\tPLoss: -131.693\tCLoss: 2.578\tEst: 131.653\n",
      "Episode 19250\tAvg: 375.350\tMin: 13.904\tMax: 963.405\talpha: 0.093\tPLoss: -133.845\tCLoss: 9.745\tEst: 133.662\n",
      "Episode 19300\tAvg: 373.468\tMin: 43.679\tMax: 1484.066\talpha: 0.101\tPLoss: -135.772\tCLoss: 16.488\tEst: 135.778\n",
      "Episode 19350\tAvg: 394.163\tMin: 22.503\tMax: 1517.283\talpha: 0.096\tPLoss: -137.147\tCLoss: 9.599\tEst: 137.521\n",
      "Episode 19400\tAvg: 359.234\tMin: 17.433\tMax: 1474.604\talpha: 0.104\tPLoss: -144.057\tCLoss: 4.888\tEst: 143.777\n",
      "Episode 19450\tAvg: 396.546\tMin: 26.952\tMax: 1393.290\talpha: 0.104\tPLoss: -139.526\tCLoss: 5.452\tEst: 139.180\n",
      "Episode 19500\tAvg: 385.201\tMin: 16.955\tMax: 976.861\talpha: 0.096\tPLoss: -141.253\tCLoss: 15.166\tEst: 141.058\n",
      "Episode 19550\tAvg: 355.081\tMin: 14.207\tMax: 1287.489\talpha: 0.102\tPLoss: -145.465\tCLoss: 2.782\tEst: 145.285\n",
      "Episode 19600\tAvg: 338.060\tMin: 10.603\tMax: 993.973\talpha: 0.101\tPLoss: -140.867\tCLoss: 11.849\tEst: 140.602\n",
      "Episode 19650\tAvg: 313.738\tMin: 3.552\tMax: 956.598\talpha: 0.105\tPLoss: -146.062\tCLoss: 4.887\tEst: 145.979\n",
      "Episode 19700\tAvg: 343.164\tMin: 26.183\tMax: 1462.344\talpha: 0.100\tPLoss: -142.248\tCLoss: 8.285\tEst: 141.952\n",
      "Episode 19750\tAvg: 342.971\tMin: 23.717\tMax: 1405.747\talpha: 0.096\tPLoss: -131.530\tCLoss: 7.475\tEst: 131.459\n",
      "Episode 19800\tAvg: 334.546\tMin: 9.417\tMax: 1258.771\talpha: 0.099\tPLoss: -142.656\tCLoss: 11.100\tEst: 142.556\n",
      "Episode 19850\tAvg: 368.403\tMin: 24.287\tMax: 1586.420\talpha: 0.100\tPLoss: -138.207\tCLoss: 7.605\tEst: 138.198\n",
      "Episode 19900\tAvg: 391.704\tMin: 27.235\tMax: 1473.286\talpha: 0.103\tPLoss: -135.464\tCLoss: 6.997\tEst: 135.212\n",
      "Episode 19950\tAvg: 431.609\tMin: 33.980\tMax: 1538.535\talpha: 0.104\tPLoss: -138.851\tCLoss: 18.000\tEst: 138.902\n",
      "Episode 20000\tAvg: 454.414\tMin: 15.224\tMax: 1291.362\talpha: 0.106\tPLoss: -156.377\tCLoss: 3.634\tEst: 156.490\n",
      "Episode 20050\tAvg: 479.141\tMin: 32.730\tMax: 1521.982\talpha: 0.121\tPLoss: -156.402\tCLoss: 5.524\tEst: 156.030\n",
      "Episode 20100\tAvg: 416.980\tMin: 11.514\tMax: 1138.354\talpha: 0.124\tPLoss: -143.227\tCLoss: 7.803\tEst: 142.440\n",
      "Episode 20150\tAvg: 406.932\tMin: 16.862\tMax: 1611.076\talpha: 0.113\tPLoss: -151.874\tCLoss: 14.123\tEst: 151.919\n",
      "Episode 20200\tAvg: 450.957\tMin: 33.415\tMax: 1416.664\talpha: 0.108\tPLoss: -131.563\tCLoss: 5.744\tEst: 131.574\n",
      "Episode 20250\tAvg: 416.904\tMin: 5.650\tMax: 1365.131\talpha: 0.112\tPLoss: -153.019\tCLoss: 5.836\tEst: 152.514\n",
      "Episode 20300\tAvg: 398.594\tMin: 9.849\tMax: 1515.847\talpha: 0.109\tPLoss: -148.209\tCLoss: 2.380\tEst: 148.220\n",
      "Episode 20350\tAvg: 405.036\tMin: 22.364\tMax: 1529.311\talpha: 0.102\tPLoss: -146.065\tCLoss: 5.497\tEst: 145.931\n",
      "Episode 20400\tAvg: 381.768\tMin: 20.078\tMax: 1311.485\talpha: 0.104\tPLoss: -144.090\tCLoss: 7.866\tEst: 144.233\n",
      "Episode 20450\tAvg: 355.774\tMin: 20.583\tMax: 1466.191\talpha: 0.107\tPLoss: -148.562\tCLoss: 5.708\tEst: 148.624\n",
      "Episode 20500\tAvg: 456.051\tMin: 28.090\tMax: 1441.464\talpha: 0.113\tPLoss: -151.600\tCLoss: 9.221\tEst: 151.477\n",
      "Episode 20550\tAvg: 474.724\tMin: 21.260\tMax: 1604.520\talpha: 0.106\tPLoss: -152.512\tCLoss: 11.832\tEst: 152.335\n",
      "Episode 20600\tAvg: 484.093\tMin: 16.998\tMax: 1681.978\talpha: 0.109\tPLoss: -153.753\tCLoss: 287.127\tEst: 153.434\n",
      "Episode 20650\tAvg: 440.368\tMin: 15.074\tMax: 1209.155\talpha: 0.101\tPLoss: -149.086\tCLoss: 11.314\tEst: 149.246\n",
      "Episode 20700\tAvg: 359.059\tMin: 18.559\tMax: 1553.830\talpha: 0.106\tPLoss: -155.461\tCLoss: 13.165\tEst: 155.284\n",
      "Episode 20750\tAvg: 410.396\tMin: 29.781\tMax: 1612.305\talpha: 0.104\tPLoss: -146.872\tCLoss: 13.316\tEst: 147.118\n",
      "Episode 20800\tAvg: 468.333\tMin: 25.767\tMax: 1506.694\talpha: 0.118\tPLoss: -150.021\tCLoss: 6.214\tEst: 149.837\n",
      "Episode 20850\tAvg: 521.577\tMin: 21.790\tMax: 1596.275\talpha: 0.103\tPLoss: -152.101\tCLoss: 4.449\tEst: 151.799\n",
      "Episode 20900\tAvg: 470.671\tMin: 44.552\tMax: 1636.139\talpha: 0.106\tPLoss: -159.956\tCLoss: 24.899\tEst: 159.701\n",
      "Episode 20950\tAvg: 429.365\tMin: 30.573\tMax: 1692.204\talpha: 0.111\tPLoss: -143.714\tCLoss: 5.174\tEst: 143.439\n",
      "Episode 21000\tAvg: 435.782\tMin: 12.883\tMax: 1413.082\talpha: 0.111\tPLoss: -150.482\tCLoss: 17.511\tEst: 149.151\n",
      "Episode 21050\tAvg: 450.187\tMin: 26.941\tMax: 1634.040\talpha: 0.113\tPLoss: -163.727\tCLoss: 2.912\tEst: 163.663\n",
      "Episode 21100\tAvg: 517.494\tMin: 137.000\tMax: 1642.199\talpha: 0.118\tPLoss: -153.186\tCLoss: 5.512\tEst: 153.349\n",
      "Episode 21150\tAvg: 549.105\tMin: 25.223\tMax: 1489.516\talpha: 0.106\tPLoss: -140.099\tCLoss: 18.596\tEst: 140.021\n",
      "Episode 21200\tAvg: 573.118\tMin: 11.059\tMax: 1658.553\talpha: 0.102\tPLoss: -155.231\tCLoss: 34.326\tEst: 154.801\n",
      "Episode 21250\tAvg: 506.299\tMin: 19.353\tMax: 1464.592\talpha: 0.105\tPLoss: -157.930\tCLoss: 3.405\tEst: 157.709\n",
      "Episode 21300\tAvg: 504.187\tMin: 22.752\tMax: 1670.956\talpha: 0.112\tPLoss: -165.845\tCLoss: 7.264\tEst: 165.782\n",
      "Episode 21350\tAvg: 543.411\tMin: 7.899\tMax: 1690.501\talpha: 0.115\tPLoss: -168.999\tCLoss: 12.483\tEst: 169.101\n",
      "Episode 21400\tAvg: 548.840\tMin: 81.423\tMax: 1656.407\talpha: 0.115\tPLoss: -155.805\tCLoss: 3.837\tEst: 155.636\n",
      "Episode 21450\tAvg: 570.822\tMin: 12.238\tMax: 1667.483\talpha: 0.117\tPLoss: -163.446\tCLoss: 4.377\tEst: 163.301\n",
      "Episode 21500\tAvg: 591.750\tMin: 18.224\tMax: 1750.490\talpha: 0.121\tPLoss: -159.904\tCLoss: 59.089\tEst: 160.654\n",
      "Episode 21550\tAvg: 559.498\tMin: 29.808\tMax: 1709.535\talpha: 0.112\tPLoss: -154.257\tCLoss: 8.732\tEst: 153.667\n",
      "Episode 21600\tAvg: 485.810\tMin: 63.512\tMax: 1477.697\talpha: 0.104\tPLoss: -152.874\tCLoss: 19.411\tEst: 152.618\n",
      "Episode 21650\tAvg: 503.694\tMin: 29.449\tMax: 1702.072\talpha: 0.115\tPLoss: -158.022\tCLoss: 8.067\tEst: 157.889\n",
      "Episode 21700\tAvg: 537.874\tMin: 16.689\tMax: 1672.787\talpha: 0.108\tPLoss: -172.199\tCLoss: 5.565\tEst: 172.353\n",
      "Episode 21750\tAvg: 508.853\tMin: 15.657\tMax: 1643.272\talpha: 0.110\tPLoss: -179.027\tCLoss: 6.315\tEst: 178.692\n",
      "Episode 21800\tAvg: 518.979\tMin: 77.990\tMax: 1879.380\talpha: 0.120\tPLoss: -161.404\tCLoss: 41.469\tEst: 161.129\n",
      "Episode 21850\tAvg: 548.752\tMin: 42.977\tMax: 1568.246\talpha: 0.109\tPLoss: -160.501\tCLoss: 10.845\tEst: 160.014\n",
      "Episode 21900\tAvg: 509.073\tMin: 16.321\tMax: 1738.104\talpha: 0.111\tPLoss: -159.935\tCLoss: 48.189\tEst: 160.234\n",
      "Episode 21950\tAvg: 490.385\tMin: 28.196\tMax: 1678.614\talpha: 0.113\tPLoss: -150.765\tCLoss: 12.047\tEst: 150.876\n",
      "Episode 22000\tAvg: 439.333\tMin: 16.262\tMax: 1025.248\talpha: 0.111\tPLoss: -169.982\tCLoss: 13.037\tEst: 169.695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 22050\tAvg: 526.429\tMin: 60.651\tMax: 1648.696\talpha: 0.106\tPLoss: -176.659\tCLoss: 6.553\tEst: 176.574\n",
      "Episode 22100\tAvg: 624.167\tMin: 36.237\tMax: 1907.172\talpha: 0.126\tPLoss: -181.807\tCLoss: 4.045\tEst: 181.015\n",
      "Episode 22150\tAvg: 534.804\tMin: 18.001\tMax: 1502.241\talpha: 0.127\tPLoss: -179.619\tCLoss: 31.183\tEst: 179.615\n",
      "Episode 22200\tAvg: 539.316\tMin: 9.881\tMax: 1664.664\talpha: 0.130\tPLoss: -165.356\tCLoss: 10.807\tEst: 164.822\n",
      "Episode 22250\tAvg: 554.482\tMin: 29.078\tMax: 1751.266\talpha: 0.117\tPLoss: -170.278\tCLoss: 14.612\tEst: 169.897\n",
      "Episode 22300\tAvg: 580.170\tMin: 22.163\tMax: 1482.407\talpha: 0.116\tPLoss: -173.965\tCLoss: 9.996\tEst: 173.657\n",
      "Episode 22350\tAvg: 581.960\tMin: 23.860\tMax: 1221.010\talpha: 0.111\tPLoss: -167.835\tCLoss: 13.827\tEst: 166.746\n",
      "Episode 22400\tAvg: 391.680\tMin: 15.070\tMax: 1191.366\talpha: 0.112\tPLoss: -150.033\tCLoss: 6.369\tEst: 149.095\n",
      "Episode 22450\tAvg: 388.087\tMin: 50.209\tMax: 1275.619\talpha: 0.120\tPLoss: -162.916\tCLoss: 15.241\tEst: 162.698\n",
      "Episode 22500\tAvg: 469.360\tMin: 27.266\tMax: 1674.105\talpha: 0.129\tPLoss: -166.657\tCLoss: 17.016\tEst: 166.385\n",
      "Episode 22550\tAvg: 521.372\tMin: 8.494\tMax: 1808.955\talpha: 0.106\tPLoss: -162.153\tCLoss: 13.521\tEst: 161.762\n",
      "Episode 22600\tAvg: 583.337\tMin: 108.752\tMax: 1396.926\talpha: 0.115\tPLoss: -168.345\tCLoss: 8.050\tEst: 168.203\n",
      "Episode 22650\tAvg: 520.525\tMin: 18.499\tMax: 1874.676\talpha: 0.120\tPLoss: -175.289\tCLoss: 10.514\tEst: 175.215\n",
      "Episode 22700\tAvg: 509.117\tMin: 38.981\tMax: 1628.766\talpha: 0.120\tPLoss: -175.272\tCLoss: 8.844\tEst: 175.132\n",
      "Episode 22750\tAvg: 490.435\tMin: 24.134\tMax: 1457.784\talpha: 0.109\tPLoss: -168.961\tCLoss: 3.275\tEst: 168.692\n",
      "Episode 22800\tAvg: 492.516\tMin: 18.676\tMax: 1965.547\talpha: 0.117\tPLoss: -177.780\tCLoss: 7.552\tEst: 177.778\n",
      "Episode 22850\tAvg: 430.700\tMin: 1.444\tMax: 1571.039\talpha: 0.138\tPLoss: -137.156\tCLoss: 32.336\tEst: 137.519\n",
      "Episode 22900\tAvg: 201.338\tMin: 5.214\tMax: 509.998\talpha: 0.158\tPLoss: -110.472\tCLoss: 20.959\tEst: 110.393\n",
      "Episode 22950\tAvg: 81.633\tMin: 4.710\tMax: 324.221\talpha: 0.138\tPLoss: -77.291\tCLoss: 41.040\tEst: 76.687\n",
      "Episode 23000\tAvg: 114.581\tMin: 11.170\tMax: 512.401\talpha: 0.097\tPLoss: -65.245\tCLoss: 3.577\tEst: 64.962\n",
      "Episode 23050\tAvg: 197.537\tMin: 3.810\tMax: 794.012\talpha: 0.094\tPLoss: -102.290\tCLoss: 33.644\tEst: 102.344\n",
      "Episode 23100\tAvg: 252.050\tMin: 13.772\tMax: 633.368\talpha: 0.109\tPLoss: -119.072\tCLoss: 29.334\tEst: 118.829\n",
      "Episode 23150\tAvg: 243.936\tMin: 6.993\tMax: 596.225\talpha: 0.121\tPLoss: -136.977\tCLoss: 9.256\tEst: 137.130\n",
      "Episode 23200\tAvg: 256.860\tMin: 7.568\tMax: 954.920\talpha: 0.127\tPLoss: -143.688\tCLoss: 11.980\tEst: 143.164\n",
      "Episode 23250\tAvg: 298.685\tMin: 6.729\tMax: 1543.912\talpha: 0.126\tPLoss: -147.870\tCLoss: 28.957\tEst: 147.843\n",
      "Episode 23300\tAvg: 300.711\tMin: 17.112\tMax: 868.276\talpha: 0.123\tPLoss: -140.204\tCLoss: 13.026\tEst: 139.923\n",
      "Episode 23350\tAvg: 282.047\tMin: 23.024\tMax: 769.251\talpha: 0.128\tPLoss: -145.649\tCLoss: 16.071\tEst: 145.261\n",
      "Episode 23400\tAvg: 346.699\tMin: 11.630\tMax: 933.391\talpha: 0.129\tPLoss: -146.945\tCLoss: 7.876\tEst: 146.104\n",
      "Episode 23450\tAvg: 414.651\tMin: 67.133\tMax: 1029.984\talpha: 0.127\tPLoss: -160.565\tCLoss: 68.138\tEst: 160.105\n",
      "Episode 23500\tAvg: 413.148\tMin: 10.337\tMax: 1430.261\talpha: 0.126\tPLoss: -166.073\tCLoss: 3.310\tEst: 165.526\n",
      "Episode 23550\tAvg: 428.766\tMin: 55.105\tMax: 1023.545\talpha: 0.128\tPLoss: -165.179\tCLoss: 38.287\tEst: 164.595\n",
      "Episode 23600\tAvg: 453.308\tMin: 52.635\tMax: 1067.881\talpha: 0.121\tPLoss: -154.894\tCLoss: 5.505\tEst: 154.623\n",
      "Episode 23650\tAvg: 444.721\tMin: 38.663\tMax: 1264.104\talpha: 0.122\tPLoss: -168.755\tCLoss: 21.483\tEst: 168.493\n",
      "Episode 23700\tAvg: 409.555\tMin: 37.536\tMax: 1644.999\talpha: 0.116\tPLoss: -158.904\tCLoss: 2.581\tEst: 158.816\n",
      "Episode 23750\tAvg: 364.400\tMin: 46.649\tMax: 829.857\talpha: 0.113\tPLoss: -146.760\tCLoss: 44.413\tEst: 146.969\n",
      "Episode 23800\tAvg: 304.068\tMin: 40.148\tMax: 601.516\talpha: 0.118\tPLoss: -138.649\tCLoss: 81.881\tEst: 139.000\n",
      "Episode 23850\tAvg: 357.480\tMin: 8.977\tMax: 1036.103\talpha: 0.125\tPLoss: -170.253\tCLoss: 13.670\tEst: 169.888\n",
      "Episode 23900\tAvg: 416.531\tMin: 48.153\tMax: 1146.179\talpha: 0.120\tPLoss: -169.649\tCLoss: 2.714\tEst: 169.308\n",
      "Episode 23950\tAvg: 424.414\tMin: 17.899\tMax: 1205.392\talpha: 0.127\tPLoss: -169.645\tCLoss: 12.733\tEst: 169.384\n",
      "Episode 24000\tAvg: 484.942\tMin: 27.715\tMax: 1724.085\talpha: 0.139\tPLoss: -174.934\tCLoss: 13.761\tEst: 175.152\n",
      "Episode 24050\tAvg: 498.503\tMin: 23.069\tMax: 1495.303\talpha: 0.131\tPLoss: -176.990\tCLoss: 10.189\tEst: 176.831\n",
      "Episode 24100\tAvg: 490.438\tMin: 113.786\tMax: 1598.624\talpha: 0.125\tPLoss: -180.114\tCLoss: 4.130\tEst: 180.365\n",
      "Episode 24150\tAvg: 481.317\tMin: 25.027\tMax: 1287.208\talpha: 0.136\tPLoss: -167.860\tCLoss: 5.617\tEst: 167.538\n",
      "Episode 24200\tAvg: 464.242\tMin: 22.325\tMax: 1156.789\talpha: 0.135\tPLoss: -157.920\tCLoss: 45.147\tEst: 157.605\n",
      "Episode 24250\tAvg: 478.695\tMin: 34.119\tMax: 1299.580\talpha: 0.139\tPLoss: -172.532\tCLoss: 14.374\tEst: 171.902\n",
      "Episode 24300\tAvg: 490.271\tMin: 71.571\tMax: 1840.759\talpha: 0.128\tPLoss: -160.307\tCLoss: 8.290\tEst: 160.446\n",
      "Episode 24350\tAvg: 502.043\tMin: 9.630\tMax: 1532.842\talpha: 0.127\tPLoss: -180.946\tCLoss: 7.825\tEst: 180.367\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-243365f85f87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_to_learn_at\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-f06d7043b459>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(agent, scores, n_episodes, train_mode, episode_start, start_to_learn_at)\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# add only to memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m          \u001b[1;31m# learn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_states\u001b[0m                                                  \u001b[1;31m# roll over states to next time step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-crawl\\agents_maddpg\\sac.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mt_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUPDATE_LOOP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                 \u001b[0mt_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_step\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRANSFER_EVERY\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt_step\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-crawl\\agents_maddpg\\sac.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritics_losses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0mnew_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_log_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_with_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\MyDev\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = train(loaded, scores, n_episodes=60000, train_mode=True, episode_start=len(scores)+1, start_to_learn_at=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24400\tAvg: 449.323\tMin: 19.725\tMax: 1487.055\talpha: 0.119\tPLoss: -171.707\tCLoss: 10.099\tEst: 171.339\n",
      "Episode 24450\tAvg: 478.351\tMin: 73.782\tMax: 1888.674\talpha: 0.135\tPLoss: -183.072\tCLoss: 16.098\tEst: 182.765\n",
      "Episode 24500\tAvg: 581.737\tMin: 13.387\tMax: 1793.168\talpha: 0.129\tPLoss: -178.102\tCLoss: 5.008\tEst: 177.959\n",
      "Episode 24550\tAvg: 570.867\tMin: 54.205\tMax: 1810.905\talpha: 0.127\tPLoss: -181.217\tCLoss: 4.325\tEst: 181.142\n",
      "Episode 24600\tAvg: 502.182\tMin: 21.262\tMax: 1239.237\talpha: 0.131\tPLoss: -185.252\tCLoss: 4.955\tEst: 184.581\n",
      "Episode 24650\tAvg: 412.710\tMin: 44.833\tMax: 892.714\talpha: 0.134\tPLoss: -179.438\tCLoss: 7.444\tEst: 179.411\n",
      "Episode 24700\tAvg: 439.143\tMin: 73.746\tMax: 1745.300\talpha: 0.130\tPLoss: -165.489\tCLoss: 27.109\tEst: 165.591\n",
      "Episode 24750\tAvg: 488.138\tMin: 10.340\tMax: 1578.800\talpha: 0.119\tPLoss: -186.340\tCLoss: 25.348\tEst: 185.846\n",
      "Episode 24800\tAvg: 527.559\tMin: 23.000\tMax: 1447.916\talpha: 0.125\tPLoss: -183.033\tCLoss: 29.267\tEst: 182.580\n",
      "Episode 24850\tAvg: 566.599\tMin: 35.273\tMax: 1536.950\talpha: 0.127\tPLoss: -178.144\tCLoss: 4.898\tEst: 177.866\n",
      "Episode 24900\tAvg: 538.114\tMin: 108.783\tMax: 1309.408\talpha: 0.132\tPLoss: -188.644\tCLoss: 17.941\tEst: 188.366\n",
      "Episode 24950\tAvg: 556.162\tMin: 85.899\tMax: 1589.497\talpha: 0.135\tPLoss: -185.745\tCLoss: 52.299\tEst: 184.890\n",
      "Episode 25000\tAvg: 653.754\tMin: 24.706\tMax: 1946.707\talpha: 0.131\tPLoss: -189.442\tCLoss: 4.680\tEst: 189.683\n",
      "Episode 25050\tAvg: 630.482\tMin: 32.780\tMax: 1704.461\talpha: 0.123\tPLoss: -187.756\tCLoss: 21.928\tEst: 187.137\n",
      "Episode 25100\tAvg: 566.567\tMin: 75.221\tMax: 1994.430\talpha: 0.134\tPLoss: -167.356\tCLoss: 18.754\tEst: 167.409\n",
      "Episode 25150\tAvg: 559.113\tMin: 26.881\tMax: 1674.006\talpha: 0.121\tPLoss: -178.797\tCLoss: 12.099\tEst: 178.463\n",
      "Episode 25200\tAvg: 509.788\tMin: 19.510\tMax: 1785.012\talpha: 0.123\tPLoss: -175.272\tCLoss: 5.242\tEst: 175.575\n",
      "Episode 25250\tAvg: 464.981\tMin: 46.935\tMax: 1294.729\talpha: 0.127\tPLoss: -175.978\tCLoss: 15.992\tEst: 176.032\n",
      "Episode 25300\tAvg: 473.526\tMin: 25.433\tMax: 1388.995\talpha: 0.130\tPLoss: -182.487\tCLoss: 6.367\tEst: 182.550\n",
      "Episode 25350\tAvg: 519.048\tMin: 17.261\tMax: 1618.671\talpha: 0.127\tPLoss: -180.439\tCLoss: 21.897\tEst: 179.737\n",
      "Episode 25400\tAvg: 559.106\tMin: 52.145\tMax: 1862.509\talpha: 0.113\tPLoss: -185.810\tCLoss: 11.433\tEst: 185.565\n",
      "Episode 25450\tAvg: 563.644\tMin: 26.645\tMax: 2023.658\talpha: 0.115\tPLoss: -193.428\tCLoss: 5.461\tEst: 192.961\n",
      "Episode 25500\tAvg: 503.225\tMin: 15.246\tMax: 1928.602\talpha: 0.133\tPLoss: -165.635\tCLoss: 18.961\tEst: 165.761\n",
      "Episode 25550\tAvg: 546.272\tMin: 31.797\tMax: 1580.303\talpha: 0.120\tPLoss: -171.151\tCLoss: 15.329\tEst: 171.051\n",
      "Episode 25600\tAvg: 619.000\tMin: 31.116\tMax: 1967.891\talpha: 0.122\tPLoss: -193.882\tCLoss: 74.411\tEst: 193.701\n",
      "Episode 25650\tAvg: 708.520\tMin: 128.000\tMax: 2008.305\talpha: 0.126\tPLoss: -195.292\tCLoss: 14.990\tEst: 195.015\n",
      "Episode 25700\tAvg: 744.884\tMin: 11.440\tMax: 1955.220\talpha: 0.131\tPLoss: -181.972\tCLoss: 17.888\tEst: 181.825\n",
      "Episode 25750\tAvg: 745.253\tMin: 85.152\tMax: 2124.450\talpha: 0.131\tPLoss: -185.620\tCLoss: 13.475\tEst: 186.152\n",
      "Episode 25800\tAvg: 735.811\tMin: 35.790\tMax: 1953.923\talpha: 0.128\tPLoss: -185.462\tCLoss: 68.077\tEst: 185.018\n",
      "Episode 25850\tAvg: 689.234\tMin: 18.013\tMax: 2174.981\talpha: 0.143\tPLoss: -198.818\tCLoss: 5.968\tEst: 198.138\n",
      "Episode 25900\tAvg: 634.420\tMin: 17.027\tMax: 1660.196\talpha: 0.141\tPLoss: -193.138\tCLoss: 5.928\tEst: 193.520\n",
      "Episode 25950\tAvg: 624.099\tMin: 6.230\tMax: 2037.126\talpha: 0.124\tPLoss: -203.219\tCLoss: 8.286\tEst: 202.309\n",
      "Episode 26000\tAvg: 727.797\tMin: 89.770\tMax: 1966.237\talpha: 0.131\tPLoss: -190.774\tCLoss: 175.941\tEst: 191.006\n",
      "Episode 26050\tAvg: 759.596\tMin: 16.368\tMax: 2077.907\talpha: 0.133\tPLoss: -195.317\tCLoss: 26.647\tEst: 194.561\n",
      "Episode 26100\tAvg: 673.898\tMin: 25.059\tMax: 1584.487\talpha: 0.138\tPLoss: -198.637\tCLoss: 4.257\tEst: 198.197\n",
      "Episode 26150\tAvg: 658.969\tMin: 16.039\tMax: 1899.793\talpha: 0.138\tPLoss: -206.955\tCLoss: 13.376\tEst: 206.183\n",
      "Episode 26200\tAvg: 635.125\tMin: 49.163\tMax: 1663.257\talpha: 0.129\tPLoss: -189.628\tCLoss: 21.270\tEst: 190.037\n",
      "Episode 26250\tAvg: 603.816\tMin: 26.302\tMax: 2043.731\talpha: 0.122\tPLoss: -183.278\tCLoss: 9.289\tEst: 183.584\n",
      "Episode 26300\tAvg: 624.857\tMin: 21.775\tMax: 2014.668\talpha: 0.119\tPLoss: -187.063\tCLoss: 4.791\tEst: 186.813\n",
      "Episode 26350\tAvg: 653.460\tMin: 16.248\tMax: 2137.076\talpha: 0.128\tPLoss: -179.492\tCLoss: 6.827\tEst: 178.714\n",
      "Episode 26400\tAvg: 612.018\tMin: 14.120\tMax: 1937.012\talpha: 0.126\tPLoss: -190.282\tCLoss: 33.406\tEst: 190.124\n",
      "Episode 26450\tAvg: 518.276\tMin: 28.391\tMax: 2006.638\talpha: 0.122\tPLoss: -183.606\tCLoss: 9.160\tEst: 183.620\n",
      "Episode 26500\tAvg: 549.533\tMin: 16.388\tMax: 2074.395\talpha: 0.140\tPLoss: -173.337\tCLoss: 38.928\tEst: 173.300\n",
      "Episode 26550\tAvg: 649.219\tMin: 23.273\tMax: 1967.747\talpha: 0.120\tPLoss: -179.670\tCLoss: 5.681\tEst: 179.100\n",
      "Episode 26600\tAvg: 693.132\tMin: 18.909\tMax: 2166.574\talpha: 0.131\tPLoss: -190.388\tCLoss: 9.045\tEst: 190.650\n",
      "Episode 26650\tAvg: 719.090\tMin: 19.717\tMax: 2102.302\talpha: 0.131\tPLoss: -190.706\tCLoss: 11.337\tEst: 190.802\n",
      "Episode 26700\tAvg: 894.338\tMin: 60.749\tMax: 2157.576\talpha: 0.125\tPLoss: -203.773\tCLoss: 8.939\tEst: 203.102\n",
      "Episode 26750\tAvg: 883.731\tMin: 33.336\tMax: 1994.570\talpha: 0.132\tPLoss: -195.386\tCLoss: 9.670\tEst: 195.500\n",
      "Episode 26800\tAvg: 704.078\tMin: 19.540\tMax: 2133.728\talpha: 0.135\tPLoss: -187.248\tCLoss: 54.900\tEst: 187.351\n",
      "Episode 26850\tAvg: 504.714\tMin: 13.423\tMax: 1707.512\talpha: 0.121\tPLoss: -209.082\tCLoss: 6.071\tEst: 208.765\n",
      "Episode 26900\tAvg: 492.591\tMin: 24.526\tMax: 2084.776\talpha: 0.133\tPLoss: -202.936\tCLoss: 11.066\tEst: 203.064\n",
      "Episode 26950\tAvg: 687.236\tMin: 12.954\tMax: 2184.908\talpha: 0.120\tPLoss: -192.286\tCLoss: 21.588\tEst: 192.016\n",
      "Episode 27000\tAvg: 726.079\tMin: 37.650\tMax: 2039.075\talpha: 0.126\tPLoss: -197.808\tCLoss: 10.526\tEst: 197.819\n",
      "Episode 27050\tAvg: 667.392\tMin: 10.809\tMax: 1870.572\talpha: 0.125\tPLoss: -192.710\tCLoss: 9.499\tEst: 192.587\n",
      "Episode 27100\tAvg: 601.576\tMin: 5.665\tMax: 2160.780\talpha: 0.131\tPLoss: -193.660\tCLoss: 3.771\tEst: 193.583\n",
      "Episode 27150\tAvg: 529.436\tMin: 9.309\tMax: 1866.076\talpha: 0.133\tPLoss: -194.658\tCLoss: 7.520\tEst: 194.232\n",
      "Episode 27200\tAvg: 501.215\tMin: 23.222\tMax: 2095.518\talpha: 0.117\tPLoss: -193.179\tCLoss: 14.246\tEst: 193.139\n",
      "Episode 27250\tAvg: 519.197\tMin: 42.426\tMax: 1743.083\talpha: 0.125\tPLoss: -192.870\tCLoss: 4.114\tEst: 192.651\n",
      "Episode 27300\tAvg: 650.238\tMin: 96.962\tMax: 2056.811\talpha: 0.117\tPLoss: -185.186\tCLoss: 10.144\tEst: 184.424\n",
      "Episode 27350\tAvg: 678.918\tMin: 19.477\tMax: 2216.734\talpha: 0.126\tPLoss: -202.917\tCLoss: 102.071\tEst: 201.634\n",
      "Episode 27400\tAvg: 683.625\tMin: 41.550\tMax: 2229.805\talpha: 0.139\tPLoss: -192.808\tCLoss: 39.673\tEst: 192.477\n",
      "Episode 27450\tAvg: 707.345\tMin: 12.575\tMax: 2166.515\talpha: 0.135\tPLoss: -197.805\tCLoss: 29.577\tEst: 197.307\n",
      "Episode 27500\tAvg: 689.612\tMin: 18.325\tMax: 2184.011\talpha: 0.135\tPLoss: -196.773\tCLoss: 5.497\tEst: 196.412\n",
      "Episode 27550\tAvg: 661.465\tMin: 16.703\tMax: 2108.099\talpha: 0.119\tPLoss: -180.410\tCLoss: 4.652\tEst: 179.694\n",
      "Episode 27600\tAvg: 631.819\tMin: 1.536\tMax: 1913.694\talpha: 0.127\tPLoss: -199.992\tCLoss: 43.568\tEst: 200.217\n",
      "Episode 27650\tAvg: 645.654\tMin: 17.228\tMax: 2106.454\talpha: 0.130\tPLoss: -155.736\tCLoss: 25.550\tEst: 155.487\n",
      "Episode 27700\tAvg: 505.647\tMin: 70.724\tMax: 1481.325\talpha: 0.138\tPLoss: -175.765\tCLoss: 19.506\tEst: 175.285\n",
      "Episode 27750\tAvg: 427.550\tMin: 18.708\tMax: 1599.286\talpha: 0.137\tPLoss: -184.058\tCLoss: 15.908\tEst: 183.518\n",
      "Episode 27800\tAvg: 478.110\tMin: 15.448\tMax: 1843.202\talpha: 0.129\tPLoss: -180.340\tCLoss: 13.431\tEst: 179.910\n",
      "Episode 27850\tAvg: 527.726\tMin: 33.310\tMax: 2125.659\talpha: 0.116\tPLoss: -193.708\tCLoss: 6.298\tEst: 193.854\n",
      "Episode 27900\tAvg: 490.245\tMin: 29.315\tMax: 1455.599\talpha: 0.131\tPLoss: -172.862\tCLoss: 5.646\tEst: 172.886\n",
      "Episode 27950\tAvg: 465.900\tMin: 28.379\tMax: 2049.926\talpha: 0.123\tPLoss: -187.947\tCLoss: 7.572\tEst: 187.377\n",
      "Episode 28000\tAvg: 491.865\tMin: 9.170\tMax: 1659.574\talpha: 0.124\tPLoss: -199.374\tCLoss: 22.542\tEst: 199.455\n",
      "Episode 28050\tAvg: 518.666\tMin: 29.727\tMax: 1527.535\talpha: 0.131\tPLoss: -184.038\tCLoss: 30.345\tEst: 184.053\n",
      "Episode 28100\tAvg: 676.731\tMin: 18.390\tMax: 2233.330\talpha: 0.131\tPLoss: -192.417\tCLoss: 7.975\tEst: 192.376\n",
      "Episode 28150\tAvg: 783.199\tMin: 110.052\tMax: 2152.334\talpha: 0.130\tPLoss: -176.226\tCLoss: 298.226\tEst: 176.190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 28200\tAvg: 674.667\tMin: 56.575\tMax: 1744.921\talpha: 0.142\tPLoss: -196.184\tCLoss: 5.811\tEst: 195.991\n",
      "Episode 28250\tAvg: 605.016\tMin: 54.834\tMax: 1434.539\talpha: 0.135\tPLoss: -194.978\tCLoss: 11.583\tEst: 194.507\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-70bb9c12075c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_to_learn_at\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplot_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-f06d7043b459>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(agent, scores, n_episodes, train_mode, episode_start, start_to_learn_at)\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# add only to memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m          \u001b[1;31m# learn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_states\u001b[0m                                                  \u001b[1;31m# roll over states to next time step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-crawl\\agents_maddpg\\sac.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[0mt_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_step\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRANSFER_EVERY\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt_step\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_to_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-crawl\\agents_maddpg\\sac.py\u001b[0m in \u001b[0;36mupdate_target\u001b[1;34m(self, tau)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mtau\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtau\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTAU\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlocal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritics_local\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritics_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0msoft_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-crawl\\agents_maddpg\\utils.py\u001b[0m in \u001b[0;36msoft_update\u001b[1;34m(local_model, target_model, tau)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# this is transferring gradually the parameters of the online Q Network to the fixed one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtarget_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_param\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mtarget_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlocal_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtarget_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mSimpleNoise\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = train(loaded, scores, n_episodes=60000, train_mode=True, episode_start=len(scores)+1, start_to_learn_at=0)\n",
    "plot_result(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 28300\tAvg: 593.443\tMin: 16.308\tMax: 2143.458\talpha: 0.139\tPLoss: -179.618\tCLoss: 15.395\tEst: 179.592\n",
      "Episode 28350\tAvg: 564.736\tMin: 48.521\tMax: 1860.572\talpha: 0.124\tPLoss: -191.683\tCLoss: 27.710\tEst: 191.031\n",
      "Episode 28400\tAvg: 538.991\tMin: 46.343\tMax: 1155.595\talpha: 0.129\tPLoss: -191.583\tCLoss: 16.037\tEst: 191.607\n",
      "Episode 28450\tAvg: 492.697\tMin: 16.739\tMax: 1498.646\talpha: 0.127\tPLoss: -181.786\tCLoss: 16.579\tEst: 181.961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\MyDev\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 28500\tAvg: nan\tMin: nan\tMax: nan\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 28550\tAvg: nan\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 28600\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 28650\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 28700\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 28750\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 28800\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 28850\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 28900\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 28950\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 29000\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 29050\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 29100\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 29150\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 29200\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 29250\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 29300\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 29350\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 29400\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 29450\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 29500\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n",
      "Episode 29550\tAvg: -0.575\tMin: -0.575\tMax: -0.575\talpha: nan\tPLoss: nan\tCLoss: nan\tEst: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-70bb9c12075c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_to_learn_at\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplot_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-f06d7043b459>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(agent, scores, n_episodes, train_mode, episode_start, start_to_learn_at)\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# add only to memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m          \u001b[1;31m# learn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_states\u001b[0m                                                  \u001b[1;31m# roll over states to next time step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-crawl\\agents_maddpg\\sac.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mt_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUPDATE_LOOP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                 \u001b[0mt_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_step\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRANSFER_EVERY\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt_step\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-crawl\\agents_maddpg\\sac.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritics_losses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\MyDev\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\MyDev\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\MyDev\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = train(loaded, scores, n_episodes=60000, train_mode=True, episode_start=len(scores)+1, start_to_learn_at=0)\n",
    "plot_result(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-81618e7d93e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloaded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"weights/SAC/eps_28150_avg_783.199.pth\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mloaded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUPDATE_EVERY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloaded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUPDATE_LOOP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_to_learn_at\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplot_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-crawl\\agents_maddpg\\storage_sac.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename, device, agent_class)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpathMemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'activation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         network = eval(checkpoint['network'])(checkpoint['state_size'], checkpoint['action_size'],\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-crawl\\agents_maddpg\\utils.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, device, buffer_size, batch_size)\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msize\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \"\"\"\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: an integer is required"
     ]
    }
   ],
   "source": [
    "loaded, scores = Storage.load(\"weights/SAC/eps_28150_avg_783.199.pth\", device)\n",
    "loaded.UPDATE_EVERY = 2\n",
    "loaded.UPDATE_LOOP = 2\n",
    "scores = train(loaded, scores, n_episodes=60000, train_mode=True, episode_start=len(scores)+1, start_to_learn_at=0)\n",
    "plot_result(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded, scores = Storage.load(\"weights/SAC/eps_28250_avg_605.016.pth\", device)\n",
    "loaded.network.eval()\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "def act(network, states, device):\n",
    "    states = torch.from_numpy(states).float().unsqueeze(0).to(device)\n",
    "    ret = network.actor.test(states).squeeze().cpu().data.numpy()\n",
    "    return ret\n",
    "\n",
    "for i in range(2):\n",
    "    while True:\n",
    "        actions = loaded.test(states)\n",
    "        env_info = env.step(np.clip(actions, -1, 1))[brain_name]  # send all actions to the environment\n",
    "        states = env_info.vector_observations                     # get next state (for each agent)\n",
    "        dones = env_info.local_done                               # see if episode finished\n",
    "        if np.any(dones):                                         # exit loop if episode finished\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665\n",
    "1. https://ai.googleblog.com/2019/01/soft-actor-critic-deep-reinforcement.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
