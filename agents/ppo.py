import numpy as np

import torch
import torch.nn.functional as F
import torch.optim as optim
from agents.model_ppo import Gaussian
import random
from agents.utils import soft_update


class PPO():
    """Interacts with and learns from the environment."""
    
    def __init__(self, network, device, 
                 LR=1e-4,
                 GRADIENT_CLIP=1, 
                 EPOCHS=4, 
                 BATCH_SIZE=32,
                GAMMA=0.99,
                GAE_TAU=0.95,
                CLIP_EPSILON=1e-1):
        self.device = device
        self.network = network
        self.optim = optim.Adam(self.network.parameters(), lr=LR)
        self.GRADIENT_CLIP = GRADIENT_CLIP
        self.EPOCHS=EPOCHS
        self.BATCH_SIZE=BATCH_SIZE
        self.GAMMA=GAMMA
        self.CLIP_EPSILON=CLIP_EPSILON
        self.GAE_TAU=GAE_TAU
    
    def save(self, file):
        torch.save(self.network.state_dict(),"weights\\" + file)
     
    def load(self, path):
        self.network.load_state_dict(torch.load(path))
    
    def act(self, states):
        """ 
        """
        states = torch.tensor(states).float().to(self.device)
        actions, log_probs, values = self.network(states)
        return actions.cpu().detach().numpy(), log_probs.cpu().detach().numpy(), values.cpu().detach().numpy()
    
    def learn(self, states, actions, log_probs, values, rewards, next_states, dones):
        """
        Noise Reduction: Collect more trajectories and when computing the gradients, use the mean gradient accross all trajectories
        Credit Assignment: 
            instead of using the total sum of the reward, use the cumulated rewards
            gradient = SUM[ SUM(advantage*Gradient(log_prob(a|s) )]
        Importance Sampling -> Surrogate:
            Network theta -> generate action a for log_prob
            Network theta'-> find out how likely is the same action a to be generated by theta'
            Instead of gradient(log_prob(a|s)) use gradient(log_prob' - log_prob)*exp() = gradient(prob'/prob)
                This is about re-weighting factor
            Need to understand Importance Sampling and the how to apply it to probabilities
        Clip Surrogate
            apply a gradient which is always less tha 1+epsilon of the ration
        """
        states = torch.tensor(states, device=self.device, dtype=torch.float32, requires_grad=False)
        actions = torch.tensor(actions, device=self.device, dtype=torch.float32, requires_grad=False)
        log_probs = torch.tensor(log_probs, device=self.device, dtype=torch.float32, requires_grad=False)
        values = torch.tensor(values, device=self.device, dtype=torch.float32, requires_grad=False)
        rewards = torch.tensor(rewards, device=self.device, dtype=torch.float32, requires_grad=False)
        next_states = torch.tensor(next_states, device=self.device, dtype=torch.float32, requires_grad=False)
        dones = 1 - torch.tensor(dones, device=self.device, dtype=torch.float32, requires_grad=False)
        
        # should the returns be 0 if the next_state is done?
        _, _, next_value = self.network(next_states[-1])
        returns = next_value

        advantages = [None] * actions.shape[0]
        returns_array = [None] * actions.shape[0]
        advantage = torch.tensor(np.zeros((actions.shape[1])), device=self.device, dtype=torch.float32)

        for i in reversed(range(states.shape[0])):
            # for Q(s,a) = V(s) + A(s,a)
            # ==> A(s,a) = Q(s,a) - V(s)
            assert returns.shape == rewards[i].shape
            assert dones[i].shape == rewards[i].shape
            returns = rewards[i] + self.GAMMA*returns*dones[i]

            # advantages.append((returns - values[i]).detach())
            returns_array[i]=returns.unsqueeze(0).detach()

            # according to the implementation of ShangTong and to the paper 
            # High Dimensional Continuous Control Using Generalized Advantage Estimation from arxiv
            assert next_value.shape == dones[i].shape
            td_error = rewards[i] + self.GAMMA * dones[i] * next_value - values[i]
            assert advantage.shape == dones[i].shape
            assert advantage.shape == td_error.shape
            next_value = values[i]
            advantage = advantage * self.GAE_TAU * self.GAMMA * dones[i] + td_error
            advantages[i] = advantage.unsqueeze(0).detach()

        del next_value

        returns_array = torch.cat(returns_array, dim=0)
        advantages = torch.cat(advantages, dim=0).unsqueeze(-1)
        advantages = (advantages - advantages.mean()) / advantages.std()
        
        # prepare states, actions, returns,
        for epoch in range(self.EPOCHS):
            # Shuffle the indices
            for indices in self.batch_indices(len(states), self.BATCH_SIZE):
                idx = torch.tensor(indices).long()
                sampled_states = states[idx]
                sampled_actions = actions[idx]         
                sampled_log_probs = log_probs[idx]
                sampled_advantages = advantages[idx]
                sampled_returns = returns_array[idx]
                
                # find out how likely the new network would have chosen the sampled_actions
                _, new_log_probs, estimated_values = self.network(sampled_states, sampled_actions)
            
                assert new_log_probs.shape == sampled_log_probs.shape
                assert sampled_advantages.shape == sampled_log_probs.shape
                ratio = (new_log_probs - sampled_log_probs).exp()
                clip = torch.clamp(ratio, 1-self.CLIP_EPSILON, 1+self.CLIP_EPSILON)
                clipped_surrogate = torch.min(ratio*sampled_advantages, clip*sampled_advantages).squeeze()

                # for some unknown sample, we are getting this torch.Size([12]) is different from torch.Size([1, 12])
                # to avoid that we will try to skeeze when different
                # this will need a proper investigation
                if estimated_values.shape != sampled_returns.shape:
                    sampled_returns = sampled_returns.squeeze();
                assert estimated_values.shape == sampled_returns.shape, str(estimated_values.shape) + " is different from " + str(sampled_returns.shape)
                value_loss = (estimated_values - sampled_returns)**2
                entropy = -(new_log_probs.exp()*sampled_log_probs).squeeze()
                
                self.optim.zero_grad()
                (-clipped_surrogate + 0.5*value_loss - 0.1*entropy).mean().backward()
                torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.GRADIENT_CLIP)
                self.optim.step()
                
                del idx, sampled_states, sampled_actions
                del sampled_log_probs, sampled_advantages, sampled_returns
                del new_log_probs, ratio, clip, clipped_surrogate, estimated_values, value_loss
        del states, actions, log_probs, values, rewards, next_states, dones, returns, advantages, returns_array
    
    def batch_indices(self, length, batch_size):
        indices = np.arange(length)
        np.random.shuffle(indices)
        for i in range(1 + length // batch_size):
            start = batch_size*i
            end = start + batch_size
            end = min(length, end)
            if start >= length:
                return
            yield indices[start:end]